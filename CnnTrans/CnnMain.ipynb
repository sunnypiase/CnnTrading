{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "# from sb3_contrib import RecurrentPPO\n",
    "# from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots\n",
    "# import pandas_ta as ta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "# from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "# import matplotlib.pyplot as plt\n",
    "# from typing import Tuple, Optional\n",
    "import math\n",
    "import mplfinance as mpf\n",
    "from readAndSortCsv import read_and_sort_csv\n",
    "from createSequences import create_sequences\n",
    "from labelSequences import precompute_label_info, get_labels_from_precomputed\n",
    "from plotData import plot_input_output_combined, plot_input_output_combined_with_label\n",
    "from trades import compute_profit, compute_sharpe_ratio, compute_trading_statistics\n",
    "from prepareScaledData import scale_X_0_1, encode_labels, compute_sample_profit, get_profitable_indices, get_flat_indices\n",
    "from ta import trend, momentum, volatility, volume\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# import glob\n",
    "\n",
    "# # Define the directory containing the CSV files\n",
    "# csv_directory = r'E:\\AICore\\CnnTrading\\CnnTrans\\Data'  # Change this to your directory path\n",
    "\n",
    "# # Define the required columns\n",
    "# required_columns = ['date', 'open', 'high', 'low', 'close', 'volume']\n",
    "\n",
    "# # Use glob to get all CSV file paths\n",
    "# csv_files = glob.glob(os.path.join(csv_directory, '*.csv'))\n",
    "\n",
    "# # Initialize an empty list to hold individual DataFrames\n",
    "# dataframes = []\n",
    "\n",
    "# # Iterate over each CSV file\n",
    "# for file in csv_files:\n",
    "#     try:\n",
    "#         # Read the CSV file\n",
    "#         df = pd.read_csv(file)\n",
    "#         # print(df.describe())\n",
    "#         # Check if all required columns are present\n",
    "#         if all(column in df.columns for column in required_columns):\n",
    "#             # Select only the required columns\n",
    "#             df = df[required_columns]\n",
    "            \n",
    "#             # Append the DataFrame to the list\n",
    "#             dataframes.append(df)\n",
    "#         else:\n",
    "#             missing = list(set(required_columns) - set(df.columns))\n",
    "#             print(f\"Skipping {file}. Missing columns: {missing}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "# # Concatenate all DataFrames\n",
    "# if dataframes:\n",
    "#     merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "#     # Convert 'date' column to datetime for proper merging\n",
    "#     merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
    "    \n",
    "#     # Drop duplicate dates if necessary (keeping the first occurrence)\n",
    "#     merged_df = merged_df.drop_duplicates(subset=['date'], keep='first')\n",
    "    \n",
    "#     # Sort by date\n",
    "#     merged_df = merged_df.sort_values(by='date', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "#     # Save the merged DataFrame to a new CSV file\n",
    "#     merged_df.to_csv('merged_output.csv', index=False)\n",
    "    \n",
    "#     print(\"CSV files have been successfully merged into 'merged_output.csv'.\")\n",
    "# else:\n",
    "#     print(\"No valid CSV files found to merge.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = 'simple1dcnn_state_dict.pth'\n",
    "required_columns = ['date', 'open', 'high', 'low', 'close', 'volume']\n",
    "file_path = r\"E:\\AICore\\CnnTrading\\CnnTrans\\merged_output.csv\"\n",
    "input_window = 150\n",
    "output_window = 15\n",
    "np.set_printoptions(formatter={'float_kind': lambda x: f'{x:.2f}'})\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data and compute X, y, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Read & sort in descending order\n",
    "df = read_and_sort_csv(file_path, required_columns)\n",
    "\n",
    "# Assuming 'df' is your DataFrame with 'open', 'high', 'low', 'close', 'volume'\n",
    "\n",
    "# Moving Averages\n",
    "df['SMA_20'] = df['close'].rolling(window=20).mean()\n",
    "df['EMA_20'] = df['close'].ewm(span=20, adjust=False).mean()\n",
    "\n",
    "# RSI\n",
    "df['RSI_14'] = momentum.RSIIndicator(close=df['close'], window=14).rsi()\n",
    "\n",
    "# MACD\n",
    "macd = trend.MACD(close=df['close'])\n",
    "df['MACD'] = macd.macd()\n",
    "df['MACD_Signal'] = macd.macd_signal()\n",
    "df['MACD_Diff'] = macd.macd_diff()\n",
    "\n",
    "# Bollinger Bands\n",
    "bollinger = volatility.BollingerBands(close=df['close'], window=20, window_dev=2)\n",
    "df['Bollinger_High'] = bollinger.bollinger_hband()\n",
    "df['Bollinger_Low'] = bollinger.bollinger_lband()\n",
    "df['Bollinger_Middle'] = bollinger.bollinger_mavg()\n",
    "\n",
    "# ATR\n",
    "df['ATR_14'] = volatility.AverageTrueRange(high=df['high'], low=df['low'], close=df['close'], window=14).average_true_range()\n",
    "\n",
    "# OBV\n",
    "df['OBV'] = volume.OnBalanceVolumeIndicator(close=df['close'], volume=df['volume']).on_balance_volume()\n",
    "\n",
    "# Stochastic Oscillator\n",
    "stochastic = momentum.StochasticOscillator(high=df['high'], low=df['low'], close=df['close'], window=14, smooth_window=3)\n",
    "df['Stochastic_%K'] = stochastic.stoch()\n",
    "df['Stochastic_%D'] = stochastic.stoch_signal()\n",
    "\n",
    "# Ichimoku Cloud\n",
    "ichimoku = trend.IchimokuIndicator(high=df['high'], low=df['low'], window1=9, window2=26, window3=52)\n",
    "df['Ichimoku_A'] = ichimoku.ichimoku_a()\n",
    "df['Ichimoku_B'] = ichimoku.ichimoku_b()\n",
    "df['Ichimoku_Base_Line'] = ichimoku.ichimoku_base_line()\n",
    "df['Ichimoku_Conversion_Line'] = ichimoku.ichimoku_conversion_line()\n",
    "\n",
    "# Handle missing values\n",
    "df.dropna(inplace=True)  # or df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Display the updated DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>SMA_20</th>\n",
       "      <th>EMA_20</th>\n",
       "      <th>RSI_14</th>\n",
       "      <th>MACD</th>\n",
       "      <th>MACD_Signal</th>\n",
       "      <th>...</th>\n",
       "      <th>Bollinger_Low</th>\n",
       "      <th>Bollinger_Middle</th>\n",
       "      <th>ATR_14</th>\n",
       "      <th>OBV</th>\n",
       "      <th>Stochastic_%K</th>\n",
       "      <th>Stochastic_%D</th>\n",
       "      <th>Ichimoku_A</th>\n",
       "      <th>Ichimoku_B</th>\n",
       "      <th>Ichimoku_Base_Line</th>\n",
       "      <th>Ichimoku_Conversion_Line</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-12-26 23:26:00</th>\n",
       "      <td>95784.21</td>\n",
       "      <td>95784.21</td>\n",
       "      <td>95674.12</td>\n",
       "      <td>95674.13</td>\n",
       "      <td>15.19492</td>\n",
       "      <td>95716.2175</td>\n",
       "      <td>95699.302191</td>\n",
       "      <td>45.450731</td>\n",
       "      <td>-37.909115</td>\n",
       "      <td>-38.206051</td>\n",
       "      <td>...</td>\n",
       "      <td>95547.006115</td>\n",
       "      <td>95716.2175</td>\n",
       "      <td>77.335429</td>\n",
       "      <td>8.003280</td>\n",
       "      <td>56.170968</td>\n",
       "      <td>56.787097</td>\n",
       "      <td>95656.8975</td>\n",
       "      <td>95678.355</td>\n",
       "      <td>95671.690</td>\n",
       "      <td>95642.105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-26 23:25:00</th>\n",
       "      <td>95812.61</td>\n",
       "      <td>95812.61</td>\n",
       "      <td>95784.20</td>\n",
       "      <td>95784.21</td>\n",
       "      <td>5.75369</td>\n",
       "      <td>95714.8850</td>\n",
       "      <td>95707.388649</td>\n",
       "      <td>56.131483</td>\n",
       "      <td>-26.418196</td>\n",
       "      <td>-35.848480</td>\n",
       "      <td>...</td>\n",
       "      <td>95548.276038</td>\n",
       "      <td>95714.8850</td>\n",
       "      <td>81.702899</td>\n",
       "      <td>13.756970</td>\n",
       "      <td>90.915198</td>\n",
       "      <td>69.241625</td>\n",
       "      <td>95663.9975</td>\n",
       "      <td>95678.355</td>\n",
       "      <td>95671.690</td>\n",
       "      <td>95656.305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-26 23:24:00</th>\n",
       "      <td>95774.26</td>\n",
       "      <td>95813.25</td>\n",
       "      <td>95774.25</td>\n",
       "      <td>95812.60</td>\n",
       "      <td>6.25812</td>\n",
       "      <td>95713.3465</td>\n",
       "      <td>95717.408778</td>\n",
       "      <td>58.394092</td>\n",
       "      <td>-14.849549</td>\n",
       "      <td>-31.648694</td>\n",
       "      <td>...</td>\n",
       "      <td>95550.997976</td>\n",
       "      <td>95713.3465</td>\n",
       "      <td>78.652692</td>\n",
       "      <td>20.015090</td>\n",
       "      <td>99.792498</td>\n",
       "      <td>82.292888</td>\n",
       "      <td>95664.1575</td>\n",
       "      <td>95678.355</td>\n",
       "      <td>95671.690</td>\n",
       "      <td>95656.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-26 23:23:00</th>\n",
       "      <td>95781.30</td>\n",
       "      <td>95783.13</td>\n",
       "      <td>95751.56</td>\n",
       "      <td>95774.26</td>\n",
       "      <td>5.32832</td>\n",
       "      <td>95711.2760</td>\n",
       "      <td>95722.823180</td>\n",
       "      <td>54.319503</td>\n",
       "      <td>-8.675029</td>\n",
       "      <td>-27.053961</td>\n",
       "      <td>...</td>\n",
       "      <td>95553.200174</td>\n",
       "      <td>95711.2760</td>\n",
       "      <td>77.394642</td>\n",
       "      <td>14.686770</td>\n",
       "      <td>87.553073</td>\n",
       "      <td>92.753589</td>\n",
       "      <td>95664.1575</td>\n",
       "      <td>95678.355</td>\n",
       "      <td>95671.690</td>\n",
       "      <td>95656.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-26 23:22:00</th>\n",
       "      <td>95779.99</td>\n",
       "      <td>95823.48</td>\n",
       "      <td>95768.03</td>\n",
       "      <td>95781.30</td>\n",
       "      <td>18.26921</td>\n",
       "      <td>95709.9570</td>\n",
       "      <td>95728.392401</td>\n",
       "      <td>54.941230</td>\n",
       "      <td>-3.176986</td>\n",
       "      <td>-22.278566</td>\n",
       "      <td>...</td>\n",
       "      <td>95554.705805</td>\n",
       "      <td>95709.9570</td>\n",
       "      <td>75.827168</td>\n",
       "      <td>32.955980</td>\n",
       "      <td>86.960554</td>\n",
       "      <td>91.435375</td>\n",
       "      <td>95693.3150</td>\n",
       "      <td>95678.355</td>\n",
       "      <td>95671.690</td>\n",
       "      <td>95714.940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:05:00</th>\n",
       "      <td>7174.71</td>\n",
       "      <td>7177.14</td>\n",
       "      <td>7173.28</td>\n",
       "      <td>7175.61</td>\n",
       "      <td>33.72500</td>\n",
       "      <td>7177.0840</td>\n",
       "      <td>7177.881957</td>\n",
       "      <td>46.602347</td>\n",
       "      <td>0.987398</td>\n",
       "      <td>1.079855</td>\n",
       "      <td>...</td>\n",
       "      <td>7169.057800</td>\n",
       "      <td>7177.0840</td>\n",
       "      <td>4.262656</td>\n",
       "      <td>-815794.501431</td>\n",
       "      <td>18.044619</td>\n",
       "      <td>28.881726</td>\n",
       "      <td>7180.0425</td>\n",
       "      <td>7179.125</td>\n",
       "      <td>7179.395</td>\n",
       "      <td>7180.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:04:00</th>\n",
       "      <td>7179.10</td>\n",
       "      <td>7179.10</td>\n",
       "      <td>7172.94</td>\n",
       "      <td>7175.25</td>\n",
       "      <td>97.36800</td>\n",
       "      <td>7177.2560</td>\n",
       "      <td>7177.631294</td>\n",
       "      <td>46.018948</td>\n",
       "      <td>0.650488</td>\n",
       "      <td>0.993981</td>\n",
       "      <td>...</td>\n",
       "      <td>7169.548131</td>\n",
       "      <td>7177.2560</td>\n",
       "      <td>4.398181</td>\n",
       "      <td>-815891.869431</td>\n",
       "      <td>15.237467</td>\n",
       "      <td>20.900453</td>\n",
       "      <td>7179.9575</td>\n",
       "      <td>7179.125</td>\n",
       "      <td>7179.395</td>\n",
       "      <td>7180.520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:03:00</th>\n",
       "      <td>7177.77</td>\n",
       "      <td>7182.60</td>\n",
       "      <td>7177.00</td>\n",
       "      <td>7181.11</td>\n",
       "      <td>69.33000</td>\n",
       "      <td>7177.5755</td>\n",
       "      <td>7177.962599</td>\n",
       "      <td>55.733313</td>\n",
       "      <td>0.846578</td>\n",
       "      <td>0.964501</td>\n",
       "      <td>...</td>\n",
       "      <td>7169.785293</td>\n",
       "      <td>7177.5755</td>\n",
       "      <td>4.609025</td>\n",
       "      <td>-815822.539431</td>\n",
       "      <td>53.891821</td>\n",
       "      <td>29.057969</td>\n",
       "      <td>7179.9575</td>\n",
       "      <td>7179.125</td>\n",
       "      <td>7179.395</td>\n",
       "      <td>7180.520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:02:00</th>\n",
       "      <td>7179.01</td>\n",
       "      <td>7179.01</td>\n",
       "      <td>7175.25</td>\n",
       "      <td>7177.93</td>\n",
       "      <td>99.42000</td>\n",
       "      <td>7177.7375</td>\n",
       "      <td>7177.959495</td>\n",
       "      <td>50.429680</td>\n",
       "      <td>0.736887</td>\n",
       "      <td>0.918978</td>\n",
       "      <td>...</td>\n",
       "      <td>7170.060114</td>\n",
       "      <td>7177.7375</td>\n",
       "      <td>4.698380</td>\n",
       "      <td>-815921.959431</td>\n",
       "      <td>32.915567</td>\n",
       "      <td>34.014952</td>\n",
       "      <td>7179.9575</td>\n",
       "      <td>7179.125</td>\n",
       "      <td>7179.395</td>\n",
       "      <td>7180.520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:01:00</th>\n",
       "      <td>7182.43</td>\n",
       "      <td>7182.44</td>\n",
       "      <td>7178.75</td>\n",
       "      <td>7179.01</td>\n",
       "      <td>70.90900</td>\n",
       "      <td>7177.9000</td>\n",
       "      <td>7178.059543</td>\n",
       "      <td>52.096940</td>\n",
       "      <td>0.728704</td>\n",
       "      <td>0.880923</td>\n",
       "      <td>...</td>\n",
       "      <td>7170.259425</td>\n",
       "      <td>7177.9000</td>\n",
       "      <td>4.684925</td>\n",
       "      <td>-815851.050431</td>\n",
       "      <td>40.039578</td>\n",
       "      <td>42.282322</td>\n",
       "      <td>7179.6050</td>\n",
       "      <td>7179.355</td>\n",
       "      <td>7179.395</td>\n",
       "      <td>7179.815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2621738 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open      high       low     close    volume  \\\n",
       "date                                                                    \n",
       "2024-12-26 23:26:00  95784.21  95784.21  95674.12  95674.13  15.19492   \n",
       "2024-12-26 23:25:00  95812.61  95812.61  95784.20  95784.21   5.75369   \n",
       "2024-12-26 23:24:00  95774.26  95813.25  95774.25  95812.60   6.25812   \n",
       "2024-12-26 23:23:00  95781.30  95783.13  95751.56  95774.26   5.32832   \n",
       "2024-12-26 23:22:00  95779.99  95823.48  95768.03  95781.30  18.26921   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "2020-01-01 00:05:00   7174.71   7177.14   7173.28   7175.61  33.72500   \n",
       "2020-01-01 00:04:00   7179.10   7179.10   7172.94   7175.25  97.36800   \n",
       "2020-01-01 00:03:00   7177.77   7182.60   7177.00   7181.11  69.33000   \n",
       "2020-01-01 00:02:00   7179.01   7179.01   7175.25   7177.93  99.42000   \n",
       "2020-01-01 00:01:00   7182.43   7182.44   7178.75   7179.01  70.90900   \n",
       "\n",
       "                         SMA_20        EMA_20     RSI_14       MACD  \\\n",
       "date                                                                  \n",
       "2024-12-26 23:26:00  95716.2175  95699.302191  45.450731 -37.909115   \n",
       "2024-12-26 23:25:00  95714.8850  95707.388649  56.131483 -26.418196   \n",
       "2024-12-26 23:24:00  95713.3465  95717.408778  58.394092 -14.849549   \n",
       "2024-12-26 23:23:00  95711.2760  95722.823180  54.319503  -8.675029   \n",
       "2024-12-26 23:22:00  95709.9570  95728.392401  54.941230  -3.176986   \n",
       "...                         ...           ...        ...        ...   \n",
       "2020-01-01 00:05:00   7177.0840   7177.881957  46.602347   0.987398   \n",
       "2020-01-01 00:04:00   7177.2560   7177.631294  46.018948   0.650488   \n",
       "2020-01-01 00:03:00   7177.5755   7177.962599  55.733313   0.846578   \n",
       "2020-01-01 00:02:00   7177.7375   7177.959495  50.429680   0.736887   \n",
       "2020-01-01 00:01:00   7177.9000   7178.059543  52.096940   0.728704   \n",
       "\n",
       "                     MACD_Signal  ...  Bollinger_Low  Bollinger_Middle  \\\n",
       "date                              ...                                    \n",
       "2024-12-26 23:26:00   -38.206051  ...   95547.006115        95716.2175   \n",
       "2024-12-26 23:25:00   -35.848480  ...   95548.276038        95714.8850   \n",
       "2024-12-26 23:24:00   -31.648694  ...   95550.997976        95713.3465   \n",
       "2024-12-26 23:23:00   -27.053961  ...   95553.200174        95711.2760   \n",
       "2024-12-26 23:22:00   -22.278566  ...   95554.705805        95709.9570   \n",
       "...                          ...  ...            ...               ...   \n",
       "2020-01-01 00:05:00     1.079855  ...    7169.057800         7177.0840   \n",
       "2020-01-01 00:04:00     0.993981  ...    7169.548131         7177.2560   \n",
       "2020-01-01 00:03:00     0.964501  ...    7169.785293         7177.5755   \n",
       "2020-01-01 00:02:00     0.918978  ...    7170.060114         7177.7375   \n",
       "2020-01-01 00:01:00     0.880923  ...    7170.259425         7177.9000   \n",
       "\n",
       "                        ATR_14            OBV  Stochastic_%K  Stochastic_%D  \\\n",
       "date                                                                          \n",
       "2024-12-26 23:26:00  77.335429       8.003280      56.170968      56.787097   \n",
       "2024-12-26 23:25:00  81.702899      13.756970      90.915198      69.241625   \n",
       "2024-12-26 23:24:00  78.652692      20.015090      99.792498      82.292888   \n",
       "2024-12-26 23:23:00  77.394642      14.686770      87.553073      92.753589   \n",
       "2024-12-26 23:22:00  75.827168      32.955980      86.960554      91.435375   \n",
       "...                        ...            ...            ...            ...   \n",
       "2020-01-01 00:05:00   4.262656 -815794.501431      18.044619      28.881726   \n",
       "2020-01-01 00:04:00   4.398181 -815891.869431      15.237467      20.900453   \n",
       "2020-01-01 00:03:00   4.609025 -815822.539431      53.891821      29.057969   \n",
       "2020-01-01 00:02:00   4.698380 -815921.959431      32.915567      34.014952   \n",
       "2020-01-01 00:01:00   4.684925 -815851.050431      40.039578      42.282322   \n",
       "\n",
       "                     Ichimoku_A  Ichimoku_B  Ichimoku_Base_Line  \\\n",
       "date                                                              \n",
       "2024-12-26 23:26:00  95656.8975   95678.355           95671.690   \n",
       "2024-12-26 23:25:00  95663.9975   95678.355           95671.690   \n",
       "2024-12-26 23:24:00  95664.1575   95678.355           95671.690   \n",
       "2024-12-26 23:23:00  95664.1575   95678.355           95671.690   \n",
       "2024-12-26 23:22:00  95693.3150   95678.355           95671.690   \n",
       "...                         ...         ...                 ...   \n",
       "2020-01-01 00:05:00   7180.0425    7179.125            7179.395   \n",
       "2020-01-01 00:04:00   7179.9575    7179.125            7179.395   \n",
       "2020-01-01 00:03:00   7179.9575    7179.125            7179.395   \n",
       "2020-01-01 00:02:00   7179.9575    7179.125            7179.395   \n",
       "2020-01-01 00:01:00   7179.6050    7179.355            7179.395   \n",
       "\n",
       "                     Ichimoku_Conversion_Line  \n",
       "date                                           \n",
       "2024-12-26 23:26:00                 95642.105  \n",
       "2024-12-26 23:25:00                 95656.305  \n",
       "2024-12-26 23:24:00                 95656.625  \n",
       "2024-12-26 23:23:00                 95656.625  \n",
       "2024-12-26 23:22:00                 95714.940  \n",
       "...                                       ...  \n",
       "2020-01-01 00:05:00                  7180.690  \n",
       "2020-01-01 00:04:00                  7180.520  \n",
       "2020-01-01 00:03:00                  7180.520  \n",
       "2020-01-01 00:02:00                  7180.520  \n",
       "2020-01-01 00:01:00                  7179.815  \n",
       "\n",
       "[2621738 rows x 22 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ((price_out_end - price_in_end) / price_in_end) * 100\n",
    "((20 - 10) / 10) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read 2621738 rows from the CSV file.\n",
      "First few rows (descending):\n",
      "                         open      high       low     close    volume  \\\n",
      "date                                                                    \n",
      "2024-12-26 23:26:00  95784.21  95784.21  95674.12  95674.13  15.19492   \n",
      "2024-12-26 23:25:00  95812.61  95812.61  95784.20  95784.21   5.75369   \n",
      "2024-12-26 23:24:00  95774.26  95813.25  95774.25  95812.60   6.25812   \n",
      "\n",
      "                         SMA_20        EMA_20     RSI_14       MACD  \\\n",
      "date                                                                  \n",
      "2024-12-26 23:26:00  95716.2175  95699.302191  45.450731 -37.909115   \n",
      "2024-12-26 23:25:00  95714.8850  95707.388649  56.131483 -26.418196   \n",
      "2024-12-26 23:24:00  95713.3465  95717.408778  58.394092 -14.849549   \n",
      "\n",
      "                     MACD_Signal  ...  Bollinger_Low  Bollinger_Middle  \\\n",
      "date                              ...                                    \n",
      "2024-12-26 23:26:00   -38.206051  ...   95547.006115        95716.2175   \n",
      "2024-12-26 23:25:00   -35.848480  ...   95548.276038        95714.8850   \n",
      "2024-12-26 23:24:00   -31.648694  ...   95550.997976        95713.3465   \n",
      "\n",
      "                        ATR_14       OBV  Stochastic_%K  Stochastic_%D  \\\n",
      "date                                                                     \n",
      "2024-12-26 23:26:00  77.335429   8.00328      56.170968      56.787097   \n",
      "2024-12-26 23:25:00  81.702899  13.75697      90.915198      69.241625   \n",
      "2024-12-26 23:24:00  78.652692  20.01509      99.792498      82.292888   \n",
      "\n",
      "                     Ichimoku_A  Ichimoku_B  Ichimoku_Base_Line  \\\n",
      "date                                                              \n",
      "2024-12-26 23:26:00  95656.8975   95678.355            95671.69   \n",
      "2024-12-26 23:25:00  95663.9975   95678.355            95671.69   \n",
      "2024-12-26 23:24:00  95664.1575   95678.355            95671.69   \n",
      "\n",
      "                     Ichimoku_Conversion_Line  \n",
      "date                                           \n",
      "2024-12-26 23:26:00                 95642.105  \n",
      "2024-12-26 23:25:00                 95656.305  \n",
      "2024-12-26 23:24:00                 95656.625  \n",
      "\n",
      "[3 rows x 22 columns]\n",
      "Last few rows (descending):\n",
      "                        open     high      low    close  volume     SMA_20  \\\n",
      "date                                                                         \n",
      "2020-01-01 00:03:00  7177.77  7182.60  7177.00  7181.11  69.330  7177.5755   \n",
      "2020-01-01 00:02:00  7179.01  7179.01  7175.25  7177.93  99.420  7177.7375   \n",
      "2020-01-01 00:01:00  7182.43  7182.44  7178.75  7179.01  70.909  7177.9000   \n",
      "\n",
      "                          EMA_20     RSI_14      MACD  MACD_Signal  ...  \\\n",
      "date                                                                ...   \n",
      "2020-01-01 00:03:00  7177.962599  55.733313  0.846578     0.964501  ...   \n",
      "2020-01-01 00:02:00  7177.959495  50.429680  0.736887     0.918978  ...   \n",
      "2020-01-01 00:01:00  7178.059543  52.096940  0.728704     0.880923  ...   \n",
      "\n",
      "                     Bollinger_Low  Bollinger_Middle    ATR_14            OBV  \\\n",
      "date                                                                            \n",
      "2020-01-01 00:03:00    7169.785293         7177.5755  4.609025 -815822.539431   \n",
      "2020-01-01 00:02:00    7170.060114         7177.7375  4.698380 -815921.959431   \n",
      "2020-01-01 00:01:00    7170.259425         7177.9000  4.684925 -815851.050431   \n",
      "\n",
      "                     Stochastic_%K  Stochastic_%D  Ichimoku_A  Ichimoku_B  \\\n",
      "date                                                                        \n",
      "2020-01-01 00:03:00      53.891821      29.057969   7179.9575    7179.125   \n",
      "2020-01-01 00:02:00      32.915567      34.014952   7179.9575    7179.125   \n",
      "2020-01-01 00:01:00      40.039578      42.282322   7179.6050    7179.355   \n",
      "\n",
      "                     Ichimoku_Base_Line  Ichimoku_Conversion_Line  \n",
      "date                                                               \n",
      "2020-01-01 00:03:00            7179.395                  7180.520  \n",
      "2020-01-01 00:02:00            7179.395                  7180.520  \n",
      "2020-01-01 00:01:00            7179.395                  7179.815  \n",
      "\n",
      "[3 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Successfully read {len(df)} rows from the CSV file.\")\n",
    "print(\"First few rows (descending):\")\n",
    "print(df.head(3))\n",
    "print(\"Last few rows (descending):\")\n",
    "print(df.tail(3))\n",
    "\n",
    "# 2) Create input/output sequences\n",
    "# data = list(zip(*create_sequences(df, input_window=input_window, output_window=output_window, step=150)))\n",
    "# np.random.shuffle(data)\n",
    "# X, y = zip(*data)\n",
    "# X, y = np.array(X), np.array(y)\n",
    "# del data\n",
    "X, y = create_sequences(df, input_window=input_window, output_window=output_window, step=output_window)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17478, 150, 22)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_labels_simple(\n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray, \n",
    "    close_idx: int = 3, \n",
    "    threshold: float = 0.03\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes labels based on the percentage difference between the last\n",
    "    closing price of the input and output windows.\n",
    "\n",
    "    Args:\n",
    "        X: Input window data, shape (num_samples, input_window, num_features).\n",
    "        y: Output window data, shape (num_samples, output_window, num_features).\n",
    "        close_idx: Index of the closing price in the feature set.\n",
    "        threshold: Percentage threshold to classify \"long\" or \"short\".\n",
    "\n",
    "    Returns:\n",
    "        labels: An array of labels (\"long\", \"short\", \"flat\"), shape (num_samples,).\n",
    "    \"\"\"\n",
    "    num_samples = X.shape[0]\n",
    "    labels = np.empty(num_samples, dtype=object)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        price_in_end = X[i, -1, close_idx]\n",
    "        price_out_end = y[i, -1, close_idx]\n",
    "\n",
    "        if price_in_end == 0:\n",
    "            labels[i] = \"flat\"  # Avoid division by zero\n",
    "            continue\n",
    "\n",
    "        pct_diff = ((price_out_end - price_in_end) / price_in_end) * 100\n",
    "\n",
    "        if pct_diff > threshold:\n",
    "            labels[i] = \"long\"\n",
    "        elif pct_diff < -threshold:\n",
    "            labels[i] = \"short\"\n",
    "        else:\n",
    "            labels[i] = \"flat\"\n",
    "\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 17478 input-output sequences.\n",
      "Input shape: (17478, 150, 22)\n",
      "Output shape: (17478, 15, 22)\n"
     ]
    }
   ],
   "source": [
    "# # labels = label_sequences(X, y, close_idx=3, volume_idx=4, alpha=0.7, beta=0.3, threshold=1.0)\n",
    "# (\n",
    "#     direction_pct_arr,\n",
    "#     range_diff_arr,\n",
    "#     p_val_arr,\n",
    "#     median_in_arr,\n",
    "#     median_out_arr\n",
    "# ) = precompute_label_info(X, y, close_idx=3, high_idx=1, low_idx=2, volume_idx=4)\n",
    "\n",
    "# labels = get_labels_from_precomputed(\n",
    "#     direction_pct_arr, range_diff_arr, p_val_arr, median_in_arr, median_out_arr,\n",
    "#     alpha=1.469647214304428, beta=0.0054738562416353255, gamma=0, threshold=0.1426995297068393\n",
    "# )\n",
    "# #  {'alpha': 1.469647214304428, 'beta': 0.0054738562416353255, 'threshold': 0.1426995297068393}\n",
    "labels = compute_labels_simple(X, y, threshold = 0.15)\n",
    "print(f\"\\nCreated {X.shape[0]} input-output sequences.\")\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "\n",
    "# 3) (Optional) Scale sequences ...\n",
    "\n",
    "# 4) (Optional) Save sequences ...\n",
    "\n",
    "# 5) Plot a sample input-output window on one chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('X.npy', X)\n",
    "# np.save('y.npy', y)\n",
    "# np.save('labels.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load X, y, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15729"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = np.load('X.npy')\n",
    "# y = np.load('y.npy')\n",
    "# labels = np.load('labels.npy',allow_pickle = True)\n",
    "val_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Splitting:\n",
      "X_train_slice shape: (13982, 150, 22)\n",
      "y_train_slice shape: (13982, 15, 22)\n",
      "labels_train_slice shape: (13982,)\n",
      "\n",
      "X_val_slice shape: (1747, 150, 22)\n",
      "y_val_slice shape: (1747, 15, 22)\n",
      "labels_val_slice shape: (1747,)\n",
      "\n",
      "X_test_slice shape: (1749, 150, 22)\n",
      "y_test_slice shape: (1749, 15, 22)\n",
      "labels_test_slice shape: (1749,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_percent = 0.8    # 80% for training\n",
    "val_percent = 0.1      # 10% for validation\n",
    "test_percent = 0.1     # 10% for testing\n",
    "\n",
    "# Ensure that the percentages sum to 1\n",
    "assert train_percent + val_percent + test_percent == 1.0, \"Percentages must sum to 1.\"\n",
    "\n",
    "# Calculate the number of samples\n",
    "total_samples = len(X)\n",
    "train_end = int(train_percent * total_samples)\n",
    "val_end = train_end + int(val_percent * total_samples)\n",
    "\n",
    "# Split the data\n",
    "X_train_slice = X[:train_end]\n",
    "y_train_slice = y[:train_end]\n",
    "labels_train_slice = labels[:train_end]\n",
    "\n",
    "X_val_slice = X[train_end:val_end]\n",
    "y_val_slice = y[train_end:val_end]\n",
    "labels_val_slice = labels[train_end:val_end]\n",
    "\n",
    "X_test_slice = X[val_end:]\n",
    "y_test_slice = y[val_end:]\n",
    "labels_test_slice = labels[val_end:]\n",
    "\n",
    "print(\"After Splitting:\")\n",
    "print(f\"X_train_slice shape: {X_train_slice.shape}\")\n",
    "print(f\"y_train_slice shape: {y_train_slice.shape}\")\n",
    "print(f\"labels_train_slice shape: {labels_train_slice.shape}\\n\")\n",
    "\n",
    "print(f\"X_val_slice shape: {X_val_slice.shape}\")\n",
    "print(f\"y_val_slice shape: {y_val_slice.shape}\")\n",
    "print(f\"labels_val_slice shape: {labels_val_slice.shape}\\n\")\n",
    "\n",
    "print(f\"X_test_slice shape: {X_test_slice.shape}\")\n",
    "print(f\"y_test_slice shape: {y_test_slice.shape}\")\n",
    "print(f\"labels_test_slice shape: {labels_test_slice.shape}\\n\")\n",
    "\n",
    "# Save the slices to disk\n",
    "# np.save('X_train_slice.npy', X_train_slice)\n",
    "# np.save('y_train_slice.npy', y_train_slice)\n",
    "# np.save('labels_train_slice.npy', labels_train_slice)\n",
    "\n",
    "# np.save('X_val_slice.npy', X_val_slice)\n",
    "# np.save('y_val_slice.npy', y_val_slice)\n",
    "# np.save('labels_val_slice.npy', labels_val_slice)\n",
    "\n",
    "# np.save('X_test_slice.npy', X_test_slice)\n",
    "# np.save('y_test_slice.npy', y_test_slice)\n",
    "# np.save('labels_test_slice.npy', labels_test_slice)\n",
    "\n",
    "# print(\"Data slices have been saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the slices from disk\n",
    "# X_train_slice = np.load('X_train_slice.npy')\n",
    "# y_train_slice = np.load('y_train_slice.npy')\n",
    "# labels_train_slice = np.load('labels_train_slice.npy')\n",
    "\n",
    "# X_val_slice = np.load('X_val_slice.npy')\n",
    "# y_val_slice = np.load('y_val_slice.npy')\n",
    "# labels_val_slice = np.load('labels_val_slice.npy')\n",
    "\n",
    "# X_test_slice = np.load('X_test_slice.npy')\n",
    "# y_test_slice = np.load('y_test_slice.npy')\n",
    "# labels_test_slice = np.load('labels_test_slice.npy')\n",
    "\n",
    "# print(\"Data slices have been loaded successfully.\")\n",
    "# # Print shapes after loading\n",
    "# print(\"After Loading:\")\n",
    "# print(f\"X_train_loaded shape: {X_train_slice.shape}\")\n",
    "# print(f\"y_train_loaded shape: {y_train_slice.shape}\")\n",
    "# print(f\"labels_train_loaded shape: {labels_train_slice.shape}\\n\")\n",
    "\n",
    "# print(f\"X_val_loaded shape: {X_val_slice.shape}\")\n",
    "# print(f\"y_val_loaded shape: {y_val_slice.shape}\")\n",
    "# print(f\"labels_val_loaded shape: {labels_val_slice.shape}\\n\")\n",
    "\n",
    "# print(f\"X_test_loaded shape: {X_test_slice.shape}\")\n",
    "# print(f\"y_test_loaded shape: {y_test_slice.shape}\")\n",
    "# print(f\"labels_test_loaded shape: {labels_test_slice.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X\n",
    "del y\n",
    "del labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13982\n",
      "1747\n",
      "1749\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_slice))\n",
    "print(len(X_val_slice))\n",
    "print(len(X_test_slice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "\n",
    "# # Precompute once\n",
    "# precomp = precompute_label_info(X, y, close_idx=3, high_idx=1, low_idx=2, volume_idx=4)\n",
    "\n",
    "# def objective(trial, objective_name=\"profit\"):\n",
    "#     \"\"\"\n",
    "#     objective_name can be \"profit\" or \"sharpe\" to switch the metric.\n",
    "#     \"\"\"\n",
    "#     # Example: fix alpha = 0, or let it vary\n",
    "#     # alpha = 0\n",
    "#     alpha = trial.suggest_float(\"alpha\", -2.0, 2.0)\n",
    "    \n",
    "#     beta = trial.suggest_float(\"beta\", -1.0, 1.0)\n",
    "#     # gamma = trial.suggest_float(\"gamma\", 0.0, 1.0)\n",
    "#     gamma = 0\n",
    "#     threshold = trial.suggest_float(\"threshold\", 0.0, 1.0)\n",
    "\n",
    "#     # Unpack precomputed\n",
    "#     direction_pct_arr, range_diff_arr, p_val_arr, median_in_arr, median_out_arr = precomp\n",
    "    \n",
    "#     # Get labels quickly\n",
    "#     labels = get_labels_from_precomputed(\n",
    "#         direction_pct_arr, range_diff_arr, p_val_arr, median_in_arr, median_out_arr,\n",
    "#         alpha=alpha, beta=beta, gamma=gamma, threshold=threshold\n",
    "#     )\n",
    "    \n",
    "#     # Evaluate performance\n",
    "#     if objective_name == \"sharpe\":\n",
    "#         score = compute_sharpe_ratio(X, y, labels)\n",
    "#         if score == 0: score = -10\n",
    "#     else:\n",
    "#         score = compute_profit(X, y, labels)\n",
    "    \n",
    "#     # We want to maximize the metric => minimize the negative\n",
    "#     return -score\n",
    "\n",
    "# # Create and run a study, e.g. for profit\n",
    "# study = optuna.create_study(direction=\"minimize\")\n",
    "# study.optimize(lambda t: objective(t, objective_name=\"profit\"), n_trials=500)\n",
    "\n",
    "# print(\"==== Profit Objective ====\")\n",
    "# print(\"Best params:\", study.best_params)\n",
    "# print(\"Max profit found:\", -study.best_value)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you want a separate run for Sharpe ratio\n",
    "# study_sharpe = optuna.create_study(direction=\"minimize\")\n",
    "# study_sharpe.optimize(lambda t: objective(t, objective_name=\"sharpe\"), n_trials=500)\n",
    "\n",
    "# print(\"==== Sharpe Objective ====\")\n",
    "# print(\"Best params:\", study_sharpe.best_params)\n",
    "# print(\"Max Sharpe found:\", -study_sharpe.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot raw data with lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_idx = 4\n",
    "# print(labels[sample_idx])\n",
    "# # plot_input_output_combined(\n",
    "# #     df_original=df,\n",
    "# #     start_idx=sample_idx,\n",
    "# #     input_window=input_window,\n",
    "# #     output_window=output_window,\n",
    "# #     title=\"Sample Input & Output on One Chart (start_idx=0)\"\n",
    "# # )\n",
    "# plot_input_output_combined_with_label(\n",
    "#     X[sample_idx],\n",
    "#     y[sample_idx],\n",
    "#     label=labels[sample_idx],\n",
    "#     title=\"Input+Output, Colored by Label\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniqueLabels(labelsToUnique):\n",
    "    unique, counts = np.unique(labelsToUnique, return_counts=True)\n",
    "\n",
    "    print(dict(zip(unique, counts)))\n",
    "\n",
    "    print(len(np.where(labelsToUnique == 'long')[0]))\n",
    "    print(len(np.where(labelsToUnique == 'short')[0]))\n",
    "    print(len(np.where(labelsToUnique == 'flat')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'flat': 7603, 'long': 3210, 'short': 3169}\n",
      "3210\n",
      "3169\n",
      "7603\n"
     ]
    }
   ],
   "source": [
    "uniqueLabels(labels_train_slice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'flat': 997, 'long': 373, 'short': 377}\n",
      "373\n",
      "377\n",
      "997\n"
     ]
    }
   ],
   "source": [
    "uniqueLabels(labels_val_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'flat': 1029, 'long': 377, 'short': 343}\n",
      "377\n",
      "343\n",
      "1029\n"
     ]
    }
   ],
   "source": [
    "uniqueLabels(labels_test_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_indexs_for_slice(x_input, y_input, labels_input):\n",
    "#     oaoao_long = []\n",
    "#     oaoao_short = []\n",
    "#     idxs = get_profitable_indices(x_input, y_input, labels_input)\n",
    "#     for i, v in enumerate(idxs):\n",
    "#         if labels_input[v] == \"short\":\n",
    "#             oaoao_short.append((v))\n",
    "#         elif labels_input[v] == \"long\":\n",
    "#             oaoao_long.append((v))\n",
    "#     minLen = min(len(oaoao_long), len(oaoao_short), 300)\n",
    "#     print(\"minlem\", minLen)\n",
    "#     flat_idxs = get_flat_indices(labels_input, minLen)\n",
    "#     oaoao_flat = random.sample(flat_idxs, minLen)\n",
    "#     print(len(oaoao_long))\n",
    "#     print(len(oaoao_short))\n",
    "#     print(len(oaoao_flat))\n",
    "#     return oaoao_long[:minLen] + oaoao_short[:minLen] + oaoao_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minlem 300\n",
      "3210\n",
      "3169\n",
      "300\n",
      "minlem 300\n",
      "373\n",
      "377\n",
      "300\n",
      "minlem 300\n",
      "377\n",
      "343\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# train_idxs = get_indexs_for_slice(X_train_slice, y_train_slice, labels_train_slice)\n",
    "# val_idxs = get_indexs_for_slice(X_val_slice, y_val_slice, labels_val_slice)\n",
    "# test_idxs = get_indexs_for_slice(X_test_slice, y_test_slice, labels_test_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.36152042358326"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_profit(X_val_slice, y_val_slice, labels_val_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13982, 150, 22)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_slice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.shuffle(range(len(X_train_slice)))\n",
    "# np.random.shuffle(val_idxs)\n",
    "# np.random.shuffle(test_idxs)\n",
    "\n",
    "# 5) Convert each to np.array\n",
    "X_train, y_train, out_train = np.array(X_train_slice), np.array(labels_train_slice), np.array(y_train_slice)\n",
    "X_val,   y_val, out_val    = np.array(X_val_slice),   np.array(labels_val_slice), np.array(y_val_slice)\n",
    "X_test,  y_test, out_test  = np.array(X_test_slice),  np.array(labels_test_slice), np.array(y_test_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3834, 150, 22)\n",
      "(477, 150, 22)\n",
      "(489, 150, 22)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "def scale_X_0_1_per_sequence(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Scales each feature (across the time steps) within each sequence of the 3D array X independently to [0, 1].\n",
    "    A new MinMaxScaler instance is applied for each feature in each sequence.\n",
    "\n",
    "    Args:\n",
    "        X: 3D numpy array of shape (num_samples, input_window, num_features).\n",
    "\n",
    "    Returns:\n",
    "        X_scaled: Scaled version of X, where each feature is independently scaled within each sequence.\n",
    "    \"\"\"\n",
    "    num_samples, input_window, num_features = X.shape\n",
    "\n",
    "    # Initialize an array to store scaled data\n",
    "    X_scaled = np.zeros_like(X)\n",
    "\n",
    "    for sample_idx in range(num_samples):\n",
    "        for feature_idx in range(num_features):\n",
    "            # Extract the time series for a single feature in a single sequence\n",
    "            feature_sequence = X[sample_idx, :, feature_idx].reshape(-1, 1)\n",
    "            \n",
    "            # Create a MinMaxScaler for this specific sequence\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            \n",
    "            # Fit and transform the feature sequence\n",
    "            scaled_sequence = scaler.fit_transform(feature_sequence)\n",
    "            \n",
    "            # Assign the scaled sequence back to the corresponding location\n",
    "            X_scaled[sample_idx, :, feature_idx] = scaled_sequence.flatten()\n",
    "\n",
    "    return X_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def add_gaussian_noise(X, noise_factor=0.05):\n",
    "    \"\"\"\n",
    "    Adds Gaussian noise to the input data.\n",
    "\n",
    "    Parameters:\n",
    "    - X: numpy array, shape (num_samples, input_window, num_features)\n",
    "    - noise_factor: float, standard deviation of the Gaussian noise\n",
    "\n",
    "    Returns:\n",
    "    - X_noisy: numpy array with added Gaussian noise, same shape as X\n",
    "    \"\"\"\n",
    "    # Generate Gaussian noise\n",
    "    noise = np.random.normal(loc=0.0, scale=noise_factor, size=X.shape)\n",
    "    \n",
    "    # Add noise to the original data\n",
    "    X_noisy = X + noise\n",
    "    \n",
    "    # Clip the values to ensure they remain within [0, 1]\n",
    "    X_noisy = np.clip(X_noisy, 0.0, 1.0)\n",
    "    \n",
    "    return X_noisy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled shape: (3834, 150, 22)\n",
      "y_train_encoded shape: (3834,)\n",
      "Sample encoded labels: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Suppose we have:\n",
    "# X_train, X_val, X_test as (num_samples, input_window, num_features)\n",
    "# y_train, y_val, y_test as string arrays of shape (num_samples,)\n",
    "\n",
    "# 1) Scale X's\n",
    "\n",
    "X_train_scaled = scale_X_0_1_per_sequence(X_train)\n",
    "# X_train_scaled = add_gaussian_noise(X_train_scaled, noise_factor=0.05)\n",
    "\n",
    "X_val_scaled = scale_X_0_1_per_sequence(X_val)\n",
    "X_test_scaled = scale_X_0_1_per_sequence(X_test)\n",
    "\n",
    "# 2) Encode labels\n",
    "y_train_encoded = encode_labels(y_train)\n",
    "y_val_encoded   = encode_labels(y_val)\n",
    "y_test_encoded  = encode_labels(y_test)\n",
    "\n",
    "print(\"X_train_scaled shape:\", X_train_scaled.shape)\n",
    "print(\"y_train_encoded shape:\", y_train_encoded.shape)\n",
    "print(\"Sample encoded labels:\", np.unique(y_train_encoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3834, 150, 22)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Median</th>\n",
       "      <th>Std Dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.501685</td>\n",
       "      <td>0.505064</td>\n",
       "      <td>0.262436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.486416</td>\n",
       "      <td>0.483654</td>\n",
       "      <td>0.265379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.518788</td>\n",
       "      <td>0.528262</td>\n",
       "      <td>0.264805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.502145</td>\n",
       "      <td>0.505713</td>\n",
       "      <td>0.261829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.145226</td>\n",
       "      <td>0.095582</td>\n",
       "      <td>0.159446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.495508</td>\n",
       "      <td>0.489934</td>\n",
       "      <td>0.309111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.496327</td>\n",
       "      <td>0.491475</td>\n",
       "      <td>0.306286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.507033</td>\n",
       "      <td>0.509868</td>\n",
       "      <td>0.238016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>0.506008</td>\n",
       "      <td>0.268735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.504492</td>\n",
       "      <td>0.505664</td>\n",
       "      <td>0.284125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.495662</td>\n",
       "      <td>0.495228</td>\n",
       "      <td>0.235170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.479579</td>\n",
       "      <td>0.460891</td>\n",
       "      <td>0.300259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.509521</td>\n",
       "      <td>0.518111</td>\n",
       "      <td>0.303385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.495508</td>\n",
       "      <td>0.489934</td>\n",
       "      <td>0.309111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.350214</td>\n",
       "      <td>0.292118</td>\n",
       "      <td>0.258140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.503399</td>\n",
       "      <td>0.503702</td>\n",
       "      <td>0.272196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.498974</td>\n",
       "      <td>0.498052</td>\n",
       "      <td>0.308256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.497660</td>\n",
       "      <td>0.494689</td>\n",
       "      <td>0.304419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.499517</td>\n",
       "      <td>0.500183</td>\n",
       "      <td>0.301749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.497996</td>\n",
       "      <td>0.504992</td>\n",
       "      <td>0.332266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500698</td>\n",
       "      <td>0.504212</td>\n",
       "      <td>0.312257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.498951</td>\n",
       "      <td>0.500665</td>\n",
       "      <td>0.290869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Feature  Min  Max      Mean    Median   Std Dev\n",
       "0         1  0.0  1.0  0.501685  0.505064  0.262436\n",
       "1         2  0.0  1.0  0.486416  0.483654  0.265379\n",
       "2         3  0.0  1.0  0.518788  0.528262  0.264805\n",
       "3         4  0.0  1.0  0.502145  0.505713  0.261829\n",
       "4         5  0.0  1.0  0.145226  0.095582  0.159446\n",
       "5         6  0.0  1.0  0.495508  0.489934  0.309111\n",
       "6         7  0.0  1.0  0.496327  0.491475  0.306286\n",
       "7         8  0.0  1.0  0.507033  0.509868  0.238016\n",
       "8         9  0.0  1.0  0.503788  0.506008  0.268735\n",
       "9        10  0.0  1.0  0.504492  0.505664  0.284125\n",
       "10       11  0.0  1.0  0.495662  0.495228  0.235170\n",
       "11       12  0.0  1.0  0.479579  0.460891  0.300259\n",
       "12       13  0.0  1.0  0.509521  0.518111  0.303385\n",
       "13       14  0.0  1.0  0.495508  0.489934  0.309111\n",
       "14       15  0.0  1.0  0.350214  0.292118  0.258140\n",
       "15       16  0.0  1.0  0.503399  0.503702  0.272196\n",
       "16       17  0.0  1.0  0.498974  0.498052  0.308256\n",
       "17       18  0.0  1.0  0.497660  0.494689  0.304419\n",
       "18       19  0.0  1.0  0.499517  0.500183  0.301749\n",
       "19       20  0.0  1.0  0.497996  0.504992  0.332266\n",
       "20       21  0.0  1.0  0.500698  0.504212  0.312257\n",
       "21       22  0.0  1.0  0.498951  0.500665  0.290869"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_stats = {\n",
    "    \"Min\": X_val_scaled.min(),\n",
    "    \"Max\": X_val_scaled.max(),\n",
    "    \"Mean\": X_val_scaled.mean(),\n",
    "    \"Median\": np.median(X_val_scaled),\n",
    "    \"Std Dev\": X_val_scaled.std(),\n",
    "}\n",
    "\n",
    "# Calculate feature-wise statistics\n",
    "feature_stats = []\n",
    "for feature in range(X_val_scaled.shape[2]):\n",
    "    feature_data = X_val_scaled[:, :, feature].flatten()\n",
    "    stats = {\n",
    "        \"Feature\": feature + 1,\n",
    "        \"Min\": feature_data.min(),\n",
    "        \"Max\": feature_data.max(),\n",
    "        \"Mean\": feature_data.mean(),\n",
    "        \"Median\": np.median(feature_data),\n",
    "        \"Std Dev\": feature_data.std(),\n",
    "    }\n",
    "    feature_stats.append(stats)\n",
    "\n",
    "feature_stats_df = pd.DataFrame(feature_stats)\n",
    "feature_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('X_train_scaled.npy', X_train_scaled)\n",
    "# np.save('X_val_scaled.npy', X_val_scaled)\n",
    "# np.save('X_test_scaled.npy', X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data validation passed! All features are scaled between 0 and 1.\n",
      "Data validation passed! All features are scaled between 0 and 1.\n",
      "Data validation passed! All features are scaled between 0 and 1.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def validate_scaled_data(X_scaled: np.ndarray, X_original: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Validates the scaled data by checking the following:\n",
    "    1. Each feature within each time step is scaled between 0 and 1.\n",
    "    2. The shape of the scaled data matches the original data.\n",
    "\n",
    "    Args:\n",
    "        X_scaled: Scaled data, should be of the same shape as X_original.\n",
    "        X_original: Original data before scaling.\n",
    "    \"\"\"\n",
    "    # 1. Check if the shape is consistent\n",
    "    if X_scaled.shape != X_original.shape:\n",
    "        raise ValueError(f\"Shape mismatch! Expected {X_original.shape}, but got {X_scaled.shape}\")\n",
    "    \n",
    "    # 2. Check if each feature is in the [0, 1] range\n",
    "    num_samples, input_window, num_features = X_scaled.shape\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        for j in range(num_features):\n",
    "            feature_min = np.min(X_scaled[i, :, j])\n",
    "            feature_max = np.max(X_scaled[i, :, j])\n",
    "            \n",
    "            if not (0 <= feature_min <= 1.001):\n",
    "                raise ValueError(f\"Min value for feature {j} in sample {i} is out of range: {feature_min}\")\n",
    "            if not (0 <= feature_max <= 1.001):\n",
    "                raise ValueError(f\"Max value for feature {j} in sample {i} is out of range: {feature_max}\")\n",
    "    \n",
    "    print(\"Data validation passed! All features are scaled between 0 and 1.\")\n",
    "    \n",
    "\n",
    "# Example usage with your data\n",
    "validate_scaled_data(X_train_scaled, X_train)\n",
    "validate_scaled_data(X_val_scaled, X_val)\n",
    "validate_scaled_data(X_test_scaled, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "def create_image_from_data(X_scaled: np.ndarray, sample_idx: int = 0) -> None:\n",
    "    \"\"\"\n",
    "    Creates an image (600x5 pixels) from the scaled data (values between 0 and 1).\n",
    "    The values are mapped from the range [0, 1] to [0, 255] for visual representation.\n",
    "\n",
    "    Args:\n",
    "        X_scaled: Scaled data array of shape (num_samples, 600, 5).\n",
    "        sample_idx: Index of the sample to visualize.\n",
    "    \"\"\"\n",
    "    # Get the selected sample (shape: 600, 5)\n",
    "    sample_data = X_scaled[sample_idx, :, :]\n",
    "\n",
    "    # Map the data from [0, 1] to [0, 255]\n",
    "    sample_data_mapped = (sample_data * 255).astype(np.uint8)\n",
    "\n",
    "    # Create the image (600x5)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(sample_data_mapped.T, cmap='gray', aspect='auto', origin='lower', interpolation='none')\n",
    "\n",
    "    # Set axis labels and title\n",
    "    plt.title(f\"Sample {sample_idx} - Scaled Data Visualization\")\n",
    "    plt.xlabel('Time Steps (600)')\n",
    "    plt.ylabel('Features (5)')\n",
    "    plt.colorbar(label='Pixel Value (0-255)')\n",
    "\n",
    "    # Show the image\n",
    "    plt.show()\n",
    "\n",
    "# Example usage to visualize the first sample\n",
    "create_image_from_data(X_test_scaled, sample_idx=-100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Simple1DCNN(nn.Module):\n",
    "    def __init__(self, num_channels=5, seq_len=600, num_classes=3, dropout_p=0.3):\n",
    "        \"\"\"\n",
    "        An improved 1D CNN for time-series classification.\n",
    "\n",
    "        Args:\n",
    "            num_channels (int): Number of input channels (features). Defaults to 5 (e.g., open/high/low/close/volume).\n",
    "            seq_len (int): Number of timesteps in each sample (e.g., 600).\n",
    "            num_classes (int): Number of output classes (e.g., 3 for 'short','flat','long').\n",
    "            dropout_p (float): Dropout probability for regularization. Defaults to 0.3.\n",
    "        \"\"\"\n",
    "        super(Simple1DCNN, self).__init__()\n",
    "        \n",
    "        # -------------------------\n",
    "        # Convolution Block 1\n",
    "        # -------------------------\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=num_channels, \n",
    "            out_channels=32,\n",
    "            kernel_size=5,         # Larger kernel for broader context\n",
    "            stride=1, \n",
    "            padding=2              # \"same\" padding\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)  # Halves sequence length: 600 -> 300\n",
    "\n",
    "        # -------------------------\n",
    "        # Convolution Block 2\n",
    "        # -------------------------\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=32, \n",
    "            out_channels=64,\n",
    "            kernel_size=5, \n",
    "            stride=1, \n",
    "            padding=2\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)  # 300 -> 150\n",
    "\n",
    "        # -------------------------\n",
    "        # Convolution Block 3\n",
    "        # -------------------------\n",
    "        self.conv3 = nn.Conv1d(\n",
    "            in_channels=64, \n",
    "            out_channels=128,\n",
    "            kernel_size=3, \n",
    "            stride=1, \n",
    "            padding=1\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2)  # 150 -> 75\n",
    "\n",
    "        # After 3 poolings, seq_len -> seq_len / 8\n",
    "        # so final sequence length = 600 / 2 / 2 / 2 = 75\n",
    "        # out channels = 128\n",
    "        # flattened size = 128 * 75 = 9600\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "\n",
    "        # self.fc = nn.Linear(128 * (seq_len // 8), num_classes)\n",
    "\n",
    "        # OPTIONAL: If you want to do global average pooling (instead of flattening),\n",
    "        # you can comment out the above fc dimension logic and do:\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch_size, channels=num_channels, seq_len)\n",
    "        \"\"\"\n",
    "        # -------------------------\n",
    "        # Block 1\n",
    "        # -------------------------\n",
    "        x = self.conv1(x)    # (batch, 32, seq_len=600)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)    # (batch, 32, 300)\n",
    "\n",
    "        # -------------------------\n",
    "        # Block 2\n",
    "        # -------------------------\n",
    "        x = self.conv2(x)    # (batch, 64, 300)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)    # (batch, 64, 150)\n",
    "\n",
    "        # -------------------------\n",
    "        # Block 3\n",
    "        # -------------------------\n",
    "        x = self.conv3(x)    # (batch, 128, 150)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool3(x)    # (batch, 128, 75)\n",
    "\n",
    "        # -------------------------\n",
    "        # Flatten or Global Pool\n",
    "        # -------------------------\n",
    "        # # 1) Flatten approach\n",
    "        # x = x.view(x.size(0), -1)  # => (batch, 128 * 75)\n",
    "        \n",
    "        # # 2) Dropout for regularization\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        # # 3) Fully connected output\n",
    "        # x = self.fc(x)  # => (batch, num_classes=3)\n",
    "\n",
    "        # OPTIONAL (Global Pooling) approach:\n",
    "        x = self.global_pool(x)  # => (batch, 128, 1)\n",
    "        x = x.squeeze(-1)        # => (batch, 128)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = Simple1DCNN(num_channels=5, seq_len=600, num_classes=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Enlarged1DCNN(nn.Module):\n",
    "    def __init__(self, num_channels=5, seq_len=600, num_classes=3, dropout_p=0.4):\n",
    "        \"\"\"\n",
    "        An enlarged 1D CNN for time-series classification with enhanced capacity and regularization.\n",
    "    \n",
    "        Args:\n",
    "            num_channels (int): Number of input channels (features). Defaults to 5 (e.g., open/high/low/close/volume).\n",
    "            seq_len (int): Number of timesteps in each sample (e.g., 600).\n",
    "            num_classes (int): Number of output classes (e.g., 3 for 'short','flat','long').\n",
    "            dropout_p (float): Dropout probability for regularization. Defaults to 0.4.\n",
    "        \"\"\"\n",
    "        super(Enlarged1DCNN, self).__init__()\n",
    "        \n",
    "        # -------------------------\n",
    "        # Convolution Block 1\n",
    "        # -------------------------\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=num_channels, \n",
    "            out_channels=64,           # Increased from 32 to 64\n",
    "            kernel_size=7,             \n",
    "            stride=1, \n",
    "            padding=3                  \n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)  # 600 -> 300\n",
    "        self.dropout1 = nn.Dropout(p=dropout_p)\n",
    "        \n",
    "        # -------------------------\n",
    "        # Convolution Block 2\n",
    "        # -------------------------\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=64, \n",
    "            out_channels=128,          # Increased from 64 to 128\n",
    "            kernel_size=5, \n",
    "            stride=1, \n",
    "            padding=2\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)  # 300 -> 150\n",
    "        self.dropout2 = nn.Dropout(p=dropout_p)\n",
    "        \n",
    "        # -------------------------\n",
    "        # Convolution Block 3\n",
    "        # -------------------------\n",
    "        self.conv3 = nn.Conv1d(\n",
    "            in_channels=128, \n",
    "            out_channels=256,          # Increased from 128 to 256\n",
    "            kernel_size=3, \n",
    "            stride=1, \n",
    "            padding=1\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2)  # 150 -> 75\n",
    "        self.dropout3 = nn.Dropout(p=dropout_p)\n",
    "        \n",
    "        # -------------------------\n",
    "        # Convolution Block 4 (Optional)\n",
    "        # -------------------------\n",
    "        self.conv4 = nn.Conv1d(\n",
    "            in_channels=256, \n",
    "            out_channels=512,          # Additional layer\n",
    "            kernel_size=3, \n",
    "            stride=1, \n",
    "            padding=1\n",
    "        )\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=2)  # 75 -> 37 (rounded down)\n",
    "        self.dropout4 = nn.Dropout(p=dropout_p)\n",
    "        \n",
    "        # -------------------------\n",
    "        # Fully Connected Layers\n",
    "        # -------------------------\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)  # Global Average Pooling\n",
    "        self.fc1 = nn.Linear(512, 256)              # Increased size\n",
    "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
    "        self.dropout_fc1 = nn.Dropout(p=dropout_p)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(128)\n",
    "        self.dropout_fc2 = nn.Dropout(p=dropout_p)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, channels, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output logits of shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # -------------------------\n",
    "        # Block 1\n",
    "        # -------------------------\n",
    "        x = self.conv1(x)       # (batch, 64, 600)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)       # (batch, 64, 300)\n",
    "        x = self.dropout1(x)\n",
    "    \n",
    "        # -------------------------\n",
    "        # Block 2\n",
    "        # -------------------------\n",
    "        x = self.conv2(x)       # (batch, 128, 300)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)       # (batch, 128, 150)\n",
    "        x = self.dropout2(x)\n",
    "    \n",
    "        # -------------------------\n",
    "        # Block 3\n",
    "        # -------------------------\n",
    "        x = self.conv3(x)       # (batch, 256, 150)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool3(x)       # (batch, 256, 75)\n",
    "        x = self.dropout3(x)\n",
    "    \n",
    "        # -------------------------\n",
    "        # Block 4 (Optional)\n",
    "        # -------------------------\n",
    "        x = self.conv4(x)       # (batch, 512, 75)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool4(x)       # (batch, 512, 37)\n",
    "        x = self.dropout4(x)\n",
    "    \n",
    "        # -------------------------\n",
    "        # Global Pooling and Fully Connected Layers\n",
    "        # -------------------------\n",
    "        x = self.global_pool(x) # (batch, 512, 1)\n",
    "        x = x.squeeze(-1)       # (batch, 512)\n",
    "        \n",
    "        x = self.fc1(x)         # (batch, 256)\n",
    "        x = self.bn_fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout_fc1(x)\n",
    "        \n",
    "        x = self.fc2(x)         # (batch, 128)\n",
    "        x = self.bn_fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout_fc2(x)\n",
    "        \n",
    "        x = self.fc3(x)         # (batch, num_classes)\n",
    "    \n",
    "        return x\n",
    "    \n",
    "model = Enlarged1DCNN(num_channels=5, seq_len=600, num_classes=3, dropout_p=0.6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, dropout_p=0.4):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # If input and output channels differ, adjust the residual connection\n",
    "        if in_channels != out_channels:\n",
    "            self.residual = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.residual = nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = self.residual(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResidualEnlarged1DCNN(nn.Module):\n",
    "    def __init__(self, num_channels=5, seq_len=600, num_classes=3, dropout_p=0.4):\n",
    "        super(ResidualEnlarged1DCNN, self).__init__()\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.conv_initial = nn.Conv1d(num_channels, 64, kernel_size=7, stride=1, padding=3)\n",
    "        self.bn_initial = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)  # 600 -> 300\n",
    "        self.dropout_initial = nn.Dropout(p=dropout_p)\n",
    "        \n",
    "        # Residual Blocks\n",
    "        self.res_block1 = ResidualBlock(64, 128, kernel_size=5, padding=2, dropout_p=dropout_p)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)  # 300 -> 150\n",
    "        self.dropout2 = nn.Dropout(p=dropout_p)\n",
    "        \n",
    "        self.res_block2 = ResidualBlock(128, 256, kernel_size=3, padding=1, dropout_p=dropout_p)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2)  # 150 -> 75\n",
    "        self.dropout3 = nn.Dropout(p=dropout_p)\n",
    "        \n",
    "        self.res_block3 = ResidualBlock(256, 512, kernel_size=3, padding=1, dropout_p=dropout_p)\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=2)  # 75 -> 37\n",
    "        self.dropout4 = nn.Dropout(p=dropout_p)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)  # (batch, 512, 1)\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
    "        self.dropout_fc1 = nn.Dropout(p=dropout_p)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(128)\n",
    "        self.dropout_fc2 = nn.Dropout(p=dropout_p)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initial convolution\n",
    "        x = self.conv_initial(x)\n",
    "        x = self.bn_initial(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout_initial(x)\n",
    "        \n",
    "        # Residual Block 1\n",
    "        x = self.res_block1(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Residual Block 2\n",
    "        x = self.res_block2(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        # Residual Block 3\n",
    "        x = self.res_block3(x)\n",
    "        x = self.pool4(x)\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.global_pool(x)  # (batch, 512, 1)\n",
    "        x = x.squeeze(-1)        # (batch, 512)\n",
    "        \n",
    "        x = self.fc1(x)          # (batch, 256)\n",
    "        x = self.bn_fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout_fc1(x)\n",
    "        \n",
    "        x = self.fc2(x)          # (batch, 128)\n",
    "        x = self.bn_fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout_fc2(x)\n",
    "        \n",
    "        x = self.fc3(x)          # (batch, num_classes)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize the residual enlarged model\n",
    "model = ResidualEnlarged1DCNN(num_channels=5, seq_len=600, num_classes=3, dropout_p=0.4)\n",
    "\n",
    "# The rest of the training setup remains the same as before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels, \n",
    "        out_channels, \n",
    "        kernel_size=3, \n",
    "        stride=1, \n",
    "        padding=1, \n",
    "        dropout_p=0.4\n",
    "    ):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, 1, padding)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # If input and output channels differ or stride !=1, adjust the residual connection\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.residual = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "        else:\n",
    "            self.residual = nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = self.residual(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResidualEnlarged1DCNN2(nn.Module):\n",
    "    def __init__(self, num_channels=5, seq_len=600, num_classes=3, dropout_p=0.4):\n",
    "        super(ResidualEnlarged1DCNN2, self).__init__()\n",
    "        \n",
    "        # Initial convolution with stride=2 for initial downsampling\n",
    "        self.conv_initial = nn.Conv1d(num_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn_initial = nn.BatchNorm1d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)  # Further downsampling\n",
    "        self.dropout_initial = nn.Dropout(p=dropout_p)\n",
    "        \n",
    "        # Residual Blocks\n",
    "        self.res_block1 = ResidualBlock(64, 128, kernel_size=5, stride=2, padding=2, dropout_p=dropout_p)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_p)\n",
    "        \n",
    "        self.res_block2 = ResidualBlock(128, 256, kernel_size=3, stride=2, padding=1, dropout_p=dropout_p)\n",
    "        self.dropout3 = nn.Dropout(p=dropout_p)\n",
    "        \n",
    "        self.res_block3 = ResidualBlock(256, 512, kernel_size=3, stride=2, padding=1, dropout_p=dropout_p)\n",
    "        self.dropout4 = nn.Dropout(p=dropout_p)\n",
    "        \n",
    "        # Fully Connected Layers with Global Avg and Max Pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)  # (batch, 512, 1)\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)  # (batch, 512, 1)\n",
    "        self.fc1 = nn.Linear(512 * 2, 256)  # Concatenated features\n",
    "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
    "        self.dropout_fc1 = nn.Dropout(p=dropout_p)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(128)\n",
    "        self.dropout_fc2 = nn.Dropout(p=dropout_p)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initial convolution\n",
    "        x = self.conv_initial(x)          # Output shape: (batch, 64, seq_len/2)\n",
    "        x = self.bn_initial(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool1(x)                 # Output shape: (batch, 64, seq_len/4)\n",
    "        x = self.dropout_initial(x)\n",
    "        \n",
    "        # Residual Block 1\n",
    "        x = self.res_block1(x)            # Output shape: (batch, 128, seq_len/8)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Residual Block 2\n",
    "        x = self.res_block2(x)            # Output shape: (batch, 256, seq_len/16)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        # Residual Block 3\n",
    "        x = self.res_block3(x)            # Output shape: (batch, 512, seq_len/32)\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "        # Global Pooling\n",
    "        avg_pooled = self.global_avg_pool(x)  # Shape: (batch, 512, 1)\n",
    "        max_pooled = self.global_max_pool(x)  # Shape: (batch, 512, 1)\n",
    "        x = torch.cat((avg_pooled, max_pooled), dim=1)  # Shape: (batch, 1024, 1)\n",
    "        x = x.squeeze(-1)                        # Shape: (batch, 1024)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        x = self.fc1(x)                          # Shape: (batch, 256)\n",
    "        x = self.bn_fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout_fc1(x)\n",
    "        \n",
    "        x = self.fc2(x)                          # Shape: (batch, 128)\n",
    "        x = self.bn_fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout_fc2(x)\n",
    "        \n",
    "        x = self.fc3(x)                          # Shape: (batch, num_classes)\n",
    "        \n",
    "        return x\n",
    "model = ResidualEnlarged1DCNN2(num_channels=5, seq_len=600, num_classes=3, dropout_p=0.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureSpecific1DCNN_MaxPooling(\n",
      "  (cnn_blocks): ModuleList(\n",
      "    (0-21): 22 x Sequential(\n",
      "      (0): Conv1d(1, 32, kernel_size=(7,), stride=(3,))\n",
      "      (1): ReLU()\n",
      "      (2): Conv1d(32, 32, kernel_size=(7,), stride=(3,))\n",
      "      (3): ReLU()\n",
      "      (4): Conv1d(32, 32, kernel_size=(7,), stride=(7,))\n",
      "      (5): ReLU()\n",
      "      (6): AdaptiveMaxPool1d(output_size=1)\n",
      "    )\n",
      "  )\n",
      "  (final_mlp): Sequential(\n",
      "    (0): Linear(in_features=704, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.4, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeatureSpecific1DCNN_MaxPooling(nn.Module):\n",
    "    def __init__(self, num_channels=5, seq_len=600, num_classes=3, dropout_p=0.4):\n",
    "        \"\"\"\n",
    "        Initializes the FeatureSpecific1DCNN model with 3 CNN layers per feature,\n",
    "        per-branch max pooling, and a final MLP.\n",
    "\n",
    "        Args:\n",
    "            num_channels (int): Number of distinct features (default: 5).\n",
    "            seq_len (int): Length of each time series sequence (default: 600).\n",
    "            num_classes (int): Number of output classes (default: 3).\n",
    "            dropout_p (float): Dropout probability (default: 0.4).\n",
    "        \"\"\"\n",
    "        super(FeatureSpecific1DCNN_MaxPooling, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.seq_len = seq_len\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        # Define the CNN blocks to reduce sequence length from 600 to 9 in 3 layers\n",
    "        self.cnn_blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(in_channels=1, out_channels=32, kernel_size=7, stride=3),  # 600 -> 198\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(in_channels=32, out_channels=32, kernel_size=7, stride=3), # 198 -> 64\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(in_channels=32, out_channels=32, kernel_size=7, stride=7), # 64 -> 9\n",
    "                nn.ReLU(),\n",
    "                nn.AdaptiveMaxPool1d(output_size=1)  # Global max pooling over the sequence length\n",
    "            ) for _ in range(num_channels)\n",
    "        ])\n",
    "\n",
    "        # Each branch will now output (batch_size, 32, 1) after pooling\n",
    "        # Flattened size per branch: 32\n",
    "        self.flattened_size_per_branch = 32\n",
    "\n",
    "        # Calculate the concatenated size from all branches\n",
    "        self.concatenated_size = num_channels * self.flattened_size_per_branch  # e.g., 5 * 32 = 160\n",
    "\n",
    "        # Define the final MLP\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(self.concatenated_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, num_channels, seq_len).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Logits for each class.\n",
    "        \"\"\"\n",
    "        # List to hold outputs from each CNN branch\n",
    "        branch_outputs = []\n",
    "\n",
    "        # Process each feature through its corresponding CNN branch\n",
    "        for i in range(self.num_channels):\n",
    "            # Extract the i-th feature: shape (batch_size, 1, seq_len)\n",
    "            feature = x[:, i].unsqueeze(1)\n",
    "            cnn_out = self.cnn_blocks[i](feature)  # Shape: (batch_size, 32, 1)\n",
    "            cnn_out = cnn_out.view(cnn_out.size(0), -1)  # Flatten: (batch_size, 32)\n",
    "            branch_outputs.append(cnn_out)\n",
    "\n",
    "        # Concatenate all branch outputs: shape (batch_size, 160)\n",
    "        concatenated = torch.cat(branch_outputs, dim=1)  # 5 branches * 32 = 160\n",
    "\n",
    "        # Pass through the final MLP\n",
    "        logits = self.final_mlp(concatenated)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "        return logits\n",
    "\n",
    "# Instantiate the modified model\n",
    "model = FeatureSpecific1DCNN_MaxPooling(num_channels=22, seq_len=600, num_classes=3, dropout_p=0.4)\n",
    "\n",
    "# Print the model architecture to verify changes\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeriesModel(\n",
      "  (conv1): Conv1d(5, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (relu): ReLU()\n",
      "  (dropout1): Dropout(p=0.8, inplace=False)\n",
      "  (conv2): Conv1d(64, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (dropout2): Dropout(p=0.4, inplace=False)\n",
      "  (fc1): Linear(in_features=2400, out_features=16, bias=True)\n",
      "  (dropout3): Dropout(p=0.4, inplace=False)\n",
      "  (fc): Linear(in_features=16, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class TimeSeriesModel(nn.Module): \n",
    "    def __init__(self, num_channels=5, seq_len=150, num_classes=3, dropout_p=0.4):\n",
    "        super(TimeSeriesModel, self).__init__()\n",
    "        \n",
    "        # First Convolutional Layer\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=num_channels,      # 5 features: open, close, high, low, volume\n",
    "            out_channels=64,               # Number of filters\n",
    "            kernel_size=3,                 # Size of the convolutional kernel\n",
    "            padding=1                      # To maintain the sequence length\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.8)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=64,      # 5 features: open, close, high, low, volume\n",
    "            out_channels=16,               # Number of filters\n",
    "            kernel_size=3,                 # Size of the convolutional kernel\n",
    "            padding=1                      # To maintain the sequence length\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "        \n",
    "        # Optionally, add more convolutional layers or pooling layers here\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.fc1 = nn.Linear(2400, 16)\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "\n",
    "        self.fc = nn.Linear(16, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)           # Apply convolution: (batch_size, 64, seq_len)\n",
    "        x = self.dropout1(x)         # Apply dropout\n",
    "        x = self.relu(x)  \n",
    "        x = self.conv2(x)           # Apply convolution: (batch_size, 64, seq_len)\n",
    "        x = self.dropout2(x)         # Apply dropout\n",
    "        x = self.relu(x)          # Apply ReLU activation\n",
    "        x = x.view(x.size(0), -1)  # Flatten to (batch_size, 64*seq_len)\n",
    "        x = self.fc1(x)              # Fully connected layer to (batch_size, num_classes)\n",
    "        x = self.dropout3(x)         # Apply dropout\n",
    "        x = self.fc(x)              # Fully connected layer to (batch_size, num_classes)\n",
    "        return x\n",
    "    \n",
    "model = TimeSeriesModel(num_channels=5, seq_len=150, num_classes=3, dropout_p=0.6)\n",
    "\n",
    "# Print the model architecture to verify changes\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridCNN(\n",
      "  (shared_conv1): Conv1d(22, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (shared_relu): ReLU()\n",
      "  (shared_dropout): Dropout1d(p=0.4, inplace=False)\n",
      "  (feature_cnns): ModuleList(\n",
      "    (0-21): 22 x Sequential(\n",
      "      (0): Conv1d(1, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (1): ReLU()\n",
      "      (2): Dropout1d(p=0.6, inplace=False)\n",
      "      (3): Conv1d(128, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (4): ReLU()\n",
      "      (5): Dropout1d(p=0.4, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=110400, out_features=256, bias=True)\n",
      "  (fc_dropout): Dropout(p=0.8, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class HybridCNN(nn.Module):\n",
    "    def __init__(self, num_features=5, seq_len=600, num_classes=3, dropout_p=0.5):\n",
    "        super(HybridCNN, self).__init__()\n",
    "        \n",
    "        # Shared Convolutional Layer\n",
    "        self.shared_conv1 = nn.Conv1d(\n",
    "            in_channels=num_features,\n",
    "            out_channels=32,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "        )\n",
    "        self.shared_relu = nn.ReLU()\n",
    "        self.shared_dropout = nn.Dropout1d(p=0.4)  # Increased dropout\n",
    "        \n",
    "        # Feature-Specific Convolutional Layers\n",
    "        self.feature_cnns = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(1, 128, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout1d(p=0.6),  # Increased dropout\n",
    "                nn.Conv1d(128, 32, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout1d(p=0.4)   # Increased dropout\n",
    "            )\n",
    "            for _ in range(num_features)\n",
    "        ])\n",
    "        \n",
    "        # Fully Connected Layers with Increased Dropout\n",
    "        self.fc1 = nn.Linear(32 * seq_len + 32 * seq_len * num_features, 256)\n",
    "        self.fc_dropout = nn.Dropout(p=0.8)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input shape: (batch_size, num_features=5, seq_len=600)\n",
    "        \"\"\"\n",
    "        # Shared Convolution\n",
    "        shared_out = self.shared_conv1(x)  # Shape: (batch_size, 64, seq_len)\n",
    "        shared_out = self.shared_relu(shared_out)\n",
    "        shared_out = self.shared_dropout(shared_out)\n",
    "        \n",
    "        # Feature-Specific Convolutions\n",
    "        feature_outputs = []\n",
    "        for i, cnn in enumerate(self.feature_cnns):\n",
    "            feature = x[:, i:i+1, :]  # Shape: (batch_size, 1, seq_len)\n",
    "            feature_out = cnn(feature)  # Shape: (batch_size, 64, seq_len)\n",
    "            feature_outputs.append(feature_out)\n",
    "        \n",
    "        # Concatenate All Outputs\n",
    "        all_features = torch.cat([shared_out] + feature_outputs, dim=1)  # Shape: (batch_size, 64 + 64*num_features, seq_len)\n",
    "        all_features = all_features.view(all_features.size(0), -1)       # Shape: (batch_size, (64 + 64*num_features)*seq_len)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        x = self.fc1(all_features)          # Shape: (batch_size, 256)\n",
    "        x = self.fc_dropout(x)              # Dropout before activation\n",
    "        x = nn.ReLU()(x)                    # Activation\n",
    "        x = self.fc2(x)                     # Shape: (batch_size, num_classes)\n",
    "        return x\n",
    "    \n",
    "model = HybridCNN(num_features=22, seq_len=150, num_classes=3, dropout_p=0.8)\n",
    "\n",
    "# Print the model architecture to verify changes\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reconstruct the model architecture\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = Simple1DCNN(num_channels=5, seq_len=600, num_classes=3)\n",
    "\n",
    "# # Load the saved state dictionary\n",
    "# model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "\n",
    "# # Move to device\n",
    "# model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Suppose X_train_scaled.shape = (19000, 600, 5)\n",
    "# and y_train_encoded.shape = (19000,)\n",
    "X_train_transposed = np.transpose(X_train_scaled, (0, 2, 1))  # (19000, 5, 600)\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # Convert numpy arrays to PyTorch tensors\n",
    "        self.X = torch.from_numpy(X).float()  # shape: (num_samples, channels, seq_len)\n",
    "        self.y = torch.from_numpy(y).long()   # shape: (num_samples,)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Return (features, label) for sample 'idx'\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = TimeSeriesDataset(X_train_transposed, y_train_encoded)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Suppose X_val_scaled.shape = (val_size, 600, 5)\n",
    "# Suppose y_val_encoded.shape = (val_size,)\n",
    "\n",
    "# 1) Transpose\n",
    "X_val_transposed = np.transpose(np.concatenate((X_val_scaled, X_test_scaled), axis=0), (0, 2, 1))  # shape: (val_size, 5, 600)\n",
    "\n",
    "# 2) Wrap in a Dataset\n",
    "val_dataset = TimeSeriesDataset(X_val_transposed, np.concatenate((y_val_encoded, y_test_encoded), axis=0))\n",
    "\n",
    "# 3) Create DataLoader (batch_size can match or differ from train)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\AICore\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.2320, Acc: 0.5198\n",
      "Epoch 2, Loss: 0.7277, Acc: 0.7008\n",
      "Epoch 3, Loss: 0.6139, Acc: 0.7569\n",
      "Epoch 4, Loss: 0.5689, Acc: 0.7760\n",
      "Epoch 5, Loss: 0.5178, Acc: 0.7934\n",
      "Validation Loss: 0.4268, Validation Acc: 0.8261\n",
      "Confusion Matrix:\n",
      "[[273  39  10]\n",
      " [ 39 248  35]\n",
      " [ 11  34 277]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       short       0.85      0.85      0.85       322\n",
      "        flat       0.77      0.77      0.77       322\n",
      "        long       0.86      0.86      0.86       322\n",
      "\n",
      "    accuracy                           0.83       966\n",
      "   macro avg       0.83      0.83      0.83       966\n",
      "weighted avg       0.83      0.83      0.83       966\n",
      "\n",
      "Learning Rate after scheduler step: 0.001000\n",
      "Epoch 6, Loss: 0.5184, Acc: 0.7981\n",
      "Epoch 7, Loss: 0.4939, Acc: 0.8101\n",
      "Epoch 8, Loss: 0.4819, Acc: 0.8130\n",
      "Epoch 9, Loss: 0.4683, Acc: 0.8166\n",
      "Epoch 10, Loss: 0.4488, Acc: 0.8208\n",
      "Validation Loss: 0.4173, Validation Acc: 0.8323\n",
      "Confusion Matrix:\n",
      "[[292  19  11]\n",
      " [ 51 219  52]\n",
      " [ 13  16 293]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       short       0.82      0.91      0.86       322\n",
      "        flat       0.86      0.68      0.76       322\n",
      "        long       0.82      0.91      0.86       322\n",
      "\n",
      "    accuracy                           0.83       966\n",
      "   macro avg       0.84      0.83      0.83       966\n",
      "weighted avg       0.84      0.83      0.83       966\n",
      "\n",
      "Learning Rate after scheduler step: 0.001000\n",
      "Epoch 11, Loss: 0.4444, Acc: 0.8250\n",
      "Epoch 12, Loss: 0.4427, Acc: 0.8315\n",
      "Epoch 13, Loss: 0.4067, Acc: 0.8367\n",
      "Epoch 14, Loss: 0.4276, Acc: 0.8310\n",
      "Epoch 15, Loss: 0.4017, Acc: 0.8435\n",
      "Validation Loss: 0.4075, Validation Acc: 0.8313\n",
      "Confusion Matrix:\n",
      "[[281  32   9]\n",
      " [ 45 228  49]\n",
      " [  9  19 294]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       short       0.84      0.87      0.86       322\n",
      "        flat       0.82      0.71      0.76       322\n",
      "        long       0.84      0.91      0.87       322\n",
      "\n",
      "    accuracy                           0.83       966\n",
      "   macro avg       0.83      0.83      0.83       966\n",
      "weighted avg       0.83      0.83      0.83       966\n",
      "\n",
      "Learning Rate after scheduler step: 0.001000\n",
      "Epoch 16, Loss: 0.4014, Acc: 0.8435\n",
      "Epoch 17, Loss: 0.3873, Acc: 0.8485\n",
      "Epoch 18, Loss: 0.3839, Acc: 0.8503\n",
      "Epoch 19, Loss: 0.3779, Acc: 0.8534\n",
      "Epoch 20, Loss: 0.3584, Acc: 0.8607\n",
      "Validation Loss: 0.4315, Validation Acc: 0.8364\n",
      "Confusion Matrix:\n",
      "[[287  27   8]\n",
      " [ 43 227  52]\n",
      " [ 10  18 294]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       short       0.84      0.89      0.87       322\n",
      "        flat       0.83      0.70      0.76       322\n",
      "        long       0.83      0.91      0.87       322\n",
      "\n",
      "    accuracy                           0.84       966\n",
      "   macro avg       0.84      0.84      0.83       966\n",
      "weighted avg       0.84      0.84      0.83       966\n",
      "\n",
      "Learning Rate after scheduler step: 0.001000\n",
      "Epoch 21, Loss: 0.3710, Acc: 0.8573\n",
      "Epoch 22, Loss: 0.3471, Acc: 0.8665\n",
      "Epoch 23, Loss: 0.3422, Acc: 0.8670\n",
      "Epoch 24, Loss: 0.3328, Acc: 0.8717\n",
      "Epoch 25, Loss: 0.3305, Acc: 0.8665\n",
      "Validation Loss: 0.4256, Validation Acc: 0.8375\n",
      "Confusion Matrix:\n",
      "[[296  21   5]\n",
      " [ 50 222  50]\n",
      " [ 12  19 291]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       short       0.83      0.92      0.87       322\n",
      "        flat       0.85      0.69      0.76       322\n",
      "        long       0.84      0.90      0.87       322\n",
      "\n",
      "    accuracy                           0.84       966\n",
      "   macro avg       0.84      0.84      0.83       966\n",
      "weighted avg       0.84      0.84      0.83       966\n",
      "\n",
      "Learning Rate after scheduler step: 0.001000\n",
      "Epoch 26, Loss: 0.3322, Acc: 0.8740\n",
      "Epoch 27, Loss: 0.3257, Acc: 0.8696\n",
      "Epoch 28, Loss: 0.2907, Acc: 0.8868\n",
      "Epoch 29, Loss: 0.3026, Acc: 0.8808\n",
      "Epoch 30, Loss: 0.2945, Acc: 0.8860\n",
      "Validation Loss: 0.4655, Validation Acc: 0.8364\n",
      "Confusion Matrix:\n",
      "[[291  25   6]\n",
      " [ 46 229  47]\n",
      " [ 10  24 288]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       short       0.84      0.90      0.87       322\n",
      "        flat       0.82      0.71      0.76       322\n",
      "        long       0.84      0.89      0.87       322\n",
      "\n",
      "    accuracy                           0.84       966\n",
      "   macro avg       0.84      0.84      0.83       966\n",
      "weighted avg       0.84      0.84      0.83       966\n",
      "\n",
      "Learning Rate after scheduler step: 0.001000\n",
      "Epoch 31, Loss: 0.2727, Acc: 0.8878\n",
      "Epoch 32, Loss: 0.2695, Acc: 0.8944\n",
      "Epoch 33, Loss: 0.2718, Acc: 0.8936\n",
      "Epoch 34, Loss: 0.2713, Acc: 0.8954\n",
      "Epoch 35, Loss: 0.2610, Acc: 0.8925\n",
      "Validation Loss: 0.4660, Validation Acc: 0.8447\n",
      "Confusion Matrix:\n",
      "[[289  28   5]\n",
      " [ 41 242  39]\n",
      " [  9  28 285]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       short       0.85      0.90      0.87       322\n",
      "        flat       0.81      0.75      0.78       322\n",
      "        long       0.87      0.89      0.88       322\n",
      "\n",
      "    accuracy                           0.84       966\n",
      "   macro avg       0.84      0.84      0.84       966\n",
      "weighted avg       0.84      0.84      0.84       966\n",
      "\n",
      "Learning Rate after scheduler step: 0.001000\n",
      "Epoch 36, Loss: 0.2486, Acc: 0.9032\n",
      "Epoch 37, Loss: 0.2617, Acc: 0.8967\n",
      "Epoch 38, Loss: 0.2498, Acc: 0.9032\n",
      "Epoch 39, Loss: 0.2431, Acc: 0.9066\n",
      "Epoch 40, Loss: 0.2269, Acc: 0.9116\n",
      "Validation Loss: 0.4980, Validation Acc: 0.8406\n",
      "Confusion Matrix:\n",
      "[[292  25   5]\n",
      " [ 47 232  43]\n",
      " [ 12  22 288]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       short       0.83      0.91      0.87       322\n",
      "        flat       0.83      0.72      0.77       322\n",
      "        long       0.86      0.89      0.88       322\n",
      "\n",
      "    accuracy                           0.84       966\n",
      "   macro avg       0.84      0.84      0.84       966\n",
      "weighted avg       0.84      0.84      0.84       966\n",
      "\n",
      "Learning Rate after scheduler step: 0.001000\n",
      "Epoch 41, Loss: 0.2399, Acc: 0.9043\n",
      "Epoch 42, Loss: 0.2255, Acc: 0.9155\n",
      "Epoch 43, Loss: 0.2203, Acc: 0.9092\n",
      "Epoch 44, Loss: 0.2144, Acc: 0.9184\n",
      "Epoch 45, Loss: 0.2119, Acc: 0.9181\n",
      "Validation Loss: 0.5286, Validation Acc: 0.8395\n",
      "Confusion Matrix:\n",
      "[[284  33   5]\n",
      " [ 39 237  46]\n",
      " [  8  24 290]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       short       0.86      0.88      0.87       322\n",
      "        flat       0.81      0.74      0.77       322\n",
      "        long       0.85      0.90      0.87       322\n",
      "\n",
      "    accuracy                           0.84       966\n",
      "   macro avg       0.84      0.84      0.84       966\n",
      "weighted avg       0.84      0.84      0.84       966\n",
      "\n",
      "Learning Rate after scheduler step: 0.001000\n",
      "Epoch 46, Loss: 0.2041, Acc: 0.9186\n",
      "Epoch 47, Loss: 0.1993, Acc: 0.9236\n",
      "Epoch 48, Loss: 0.2052, Acc: 0.9160\n",
      "Epoch 49, Loss: 0.1905, Acc: 0.9259\n",
      "Epoch 50, Loss: 0.1874, Acc: 0.9291\n",
      "Validation Loss: 0.5790, Validation Acc: 0.8302\n",
      "Confusion Matrix:\n",
      "[[295  22   5]\n",
      " [ 57 219  46]\n",
      " [  9  25 288]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       short       0.82      0.92      0.86       322\n",
      "        flat       0.82      0.68      0.74       322\n",
      "        long       0.85      0.89      0.87       322\n",
      "\n",
      "    accuracy                           0.83       966\n",
      "   macro avg       0.83      0.83      0.83       966\n",
      "weighted avg       0.83      0.83      0.83       966\n",
      "\n",
      "Learning Rate after scheduler step: 0.001000\n",
      "Epoch 51, Loss: 0.1876, Acc: 0.9285\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     46\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 48\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m X_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     49\u001b[0m _, pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     50\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (pred \u001b[38;5;241m==\u001b[39m y_batch)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Assume train_loader and val_loader are predefined DataLoader instances\n",
    "# Also assume that the model is defined and instantiated as `model`\n",
    "\n",
    "# Initialize the device, model, criterion, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "l2_lambda = 1e-4  # You can adjust this value based on your needs\n",
    "\n",
    "# Initialize the optimizer with weight_decay for L2 regularization\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=l2_lambda)\n",
    "\n",
    "# Initialize the learning rate scheduler\n",
    "# Here, ReduceLROnPlateau reduces the LR by a factor of 0.1 if validation loss doesn't improve for 5 epochs\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=15, verbose=True)\n",
    "\n",
    "# Set the desired validation loss threshold\n",
    "validation_threshold = 0.25\n",
    "\n",
    "epoch_loss = 1\n",
    "epoch = 0\n",
    "\n",
    "while True:\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Training loop\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "        _, pred = torch.max(outputs, 1)\n",
    "        correct += (pred == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    epoch += 1\n",
    "    \n",
    "    print(f\"Epoch {epoch}, Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}\")\n",
    "    \n",
    "    # Validation every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # To collect predictions and true labels for further metrics\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                outputs = model(X_batch)              # shape: (batch_size, num_classes)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "                # Predictions\n",
    "                _, predicted = torch.max(outputs, 1)  # shape: (batch_size,)\n",
    "                \n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "                total += y_batch.size(0)\n",
    "                \n",
    "                # Store predictions & labels for confusion matrix, etc.\n",
    "                all_preds.append(predicted.cpu().numpy())\n",
    "                all_labels.append(y_batch.cpu().numpy())\n",
    "\n",
    "        # Convert lists of arrays into a single 1D array\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "\n",
    "        val_loss /= total\n",
    "        val_acc = correct / total\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # Additional Metrics\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # 1) Confusion Matrix\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "\n",
    "        # 2) Classification Report\n",
    "        #    If you have 3 classes: 0=\"short\", 1=\"flat\", 2=\"long\" (example)\n",
    "        target_names = [\"short\", \"flat\", \"long\"]  # adjust if needed\n",
    "        report = classification_report(all_labels, all_preds, target_names=target_names)\n",
    "        print(\"Classification Report:\")\n",
    "        print(report)\n",
    "        \n",
    "        # Step the scheduler with the validation loss\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Learning Rate after scheduler step: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # If the validation loss is acceptable, stop training\n",
    "        if val_loss <= validation_threshold or optimizer.param_groups[0]['lr'] < 1e-8:\n",
    "            print(f\"Validation loss has reached the threshold of {validation_threshold:.4f}, stopping training.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "print(f\"Final model saved to {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [\"short\", \"flat\", \"long\"]  # Adjust based on your class names\n",
    "report = classification_report(all_labels, all_preds, target_names=target_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5500, Validation Acc: 0.8406\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       short       0.84      0.91      0.88       322\n",
      "        flat       0.85      0.69      0.76       322\n",
      "        long       0.83      0.92      0.87       322\n",
      "\n",
      "    accuracy                           0.84       966\n",
      "   macro avg       0.84      0.84      0.84       966\n",
      "weighted avg       0.84      0.84      0.84       966\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "model.eval()\n",
    "val_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# To collect predictions and true labels for further metrics\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model(X_batch)              # shape: (batch_size, num_classes)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        val_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        # Predictions\n",
    "        _, predicted = torch.max(outputs, 1)  # shape: (batch_size,)\n",
    "        \n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "        \n",
    "        # Store predictions & labels for confusion matrix, etc.\n",
    "        all_preds.append(predicted.cpu().numpy())\n",
    "        all_labels.append(y_batch.cpu().numpy())\n",
    "\n",
    "# Convert lists of arrays into a single 1D array\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "val_loss /= total\n",
    "val_acc = correct / total\n",
    "\n",
    "print(f\"Validation Loss: {val_loss:.4f}, Validation Acc: {val_acc:.4f}\")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Additional Metrics\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "target_names = [\"short\", \"flat\", \"long\"]  # adjust if needed\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names) \n",
    "disp.plot() \n",
    "# And show it: \n",
    "plt.show()\n",
    "# 2) Classification Report\n",
    "#    If you have 3 classes: 0=\"short\", 1=\"flat\", 2=\"long\" (example)\n",
    "report = classification_report(all_labels, all_preds, target_names=target_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose X_test_scaled.shape = (test_size, 600, 5)\n",
    "# Suppose y_test_encoded.shape = (test_size,)\n",
    "\n",
    "# 1) Transpose\n",
    "X_test_transposed = np.transpose(X_test_scaled, (0, 2, 1))  # shape: (test_size, 5, 600)\n",
    "\n",
    "# 2) Wrap in a Dataset\n",
    "test_dataset = TimeSeriesDataset(X_test_transposed, y_test_encoded)\n",
    "\n",
    "# 3) Create DataLoader (batch_size can match or differ from train)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5729, Validation Acc: 0.8323\n",
      "Confusion Matrix:\n",
      "[[148  12   3]\n",
      " [ 24 111  28]\n",
      " [  5  10 148]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       short       0.84      0.91      0.87       163\n",
      "        flat       0.83      0.68      0.75       163\n",
      "        long       0.83      0.91      0.87       163\n",
      "\n",
      "    accuracy                           0.83       489\n",
      "   macro avg       0.83      0.83      0.83       489\n",
      "weighted avg       0.83      0.83      0.83       489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "model.eval()\n",
    "val_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# To collect predictions and true labels for further metrics\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model(X_batch)              # shape: (batch_size, num_classes)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        val_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        # Predictions\n",
    "        _, predicted = torch.max(outputs, 1)  # shape: (batch_size,)\n",
    "        \n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "        \n",
    "        # Store predictions & labels for confusion matrix, etc.\n",
    "        all_preds.append(predicted.cpu().numpy())\n",
    "        all_labels.append(y_batch.cpu().numpy())\n",
    "\n",
    "# Convert lists of arrays into a single 1D array\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "\n",
    "val_loss /= total\n",
    "val_acc = correct / total\n",
    "\n",
    "print(f\"Validation Loss: {val_loss:.4f}, Validation Acc: {val_acc:.4f}\")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Additional Metrics\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# 1) Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# 2) Classification Report\n",
    "#    If you have 3 classes: 0=\"short\", 1=\"flat\", 2=\"long\" (example)\n",
    "target_names = [\"short\", \"flat\", \"long\"]  # adjust if needed\n",
    "report = classification_report(all_labels, all_preds, target_names=target_names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do inference on a single sample from X_test\n",
    "predicted_labels = []\n",
    "model.eval()\n",
    "\n",
    "for X_single in X_test_slice:\n",
    "    X_single_scaled = MinMaxScaler().fit_transform(X_single)  # shape (600,5)\n",
    "    X_single_scaled = np.expand_dims(X_single_scaled, axis=0)  # => (1,600,5)\n",
    "    X_single_transposed = np.transpose(X_single_scaled, (0, 2, 1))  # => (1,5,600)\n",
    "    X_single_tensor = torch.from_numpy(X_single_transposed).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(X_single_tensor)   # => shape (1,3)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        predicted_label = predicted.item()  # 0,1,2\n",
    "\n",
    "    label_map = {0:\"short\",1:\"flat\",2:\"long\"}\n",
    "    predicted_labels.append(label_map[predicted_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{False: 466, True: 1283}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kek = labels_test_slice == predicted_labels\n",
    "unique, counts = np.unique(kek, return_counts=True)\n",
    "\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200.40625"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "288585/60/ 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TradingStatisticDTO(initial_capital=500, final_capital=590.4787661114784, total_profit=90.47876611147844, average_profit=0.05173171304258344, return_on_investment=18.095753222295688, num_trades=1749, long_trades=164, short_trades=166, flat_trades=1419, position_size_per_trade=100)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_trading_statistics(X_test_slice, y_test_slice, labels_test_slice,  500, 100)\n",
    "# compute_profit(X, y, predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TradingStatisticDTO(initial_capital=500, final_capital=608.546619174336, total_profit=108.54661917433604, average_profit=0.062062103587384815, return_on_investment=21.70932383486721, num_trades=1749, long_trades=385, short_trades=356, flat_trades=1008, position_size_per_trade=100)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_trading_statistics(X_test_slice, y_test_slice, predicted_labels, 500, 100)\n",
    "# compute_profit(X, y, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_input_output_combined_with_label(\n",
    "    input_sequence: np.ndarray,\n",
    "    output_sequence: np.ndarray,\n",
    "    label: str,\n",
    "    title: str = \"Input + Output Window on One Chart\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots both input and output windows on a single candlestick chart.\n",
    "    The output window candles will be overlaid in a color depending on 'label':\n",
    "      - long -> green\n",
    "      - short -> red\n",
    "      - flat -> blue\n",
    "\n",
    "    Args:\n",
    "        input_sequence (np.ndarray): OHLCV data for the input window (shape: [N, >=5])\n",
    "        output_sequence (np.ndarray): OHLCV data for the output window (shape: [M, >=5])\n",
    "        label (str): The trading signal for the output window ('long', 'short', 'flat').\n",
    "        title (str): The title of the chart.\n",
    "    \"\"\"\n",
    "\n",
    "    # Use only the first 5 columns (OHLCV)\n",
    "    input_sequence = input_sequence[:, :5]\n",
    "    output_sequence = output_sequence[:, :5]\n",
    "\n",
    "    # Generate a continuous datetime index for plotting\n",
    "    input_dates = pd.date_range(start=\"2023-01-01\", periods=len(input_sequence), freq=\"min\")\n",
    "    output_dates = pd.date_range(start=input_dates[-1] + pd.Timedelta(minutes=1), periods=len(output_sequence), freq=\"min\")\n",
    "\n",
    "    # Convert to DataFrames with appropriate columns\n",
    "    input_df = pd.DataFrame(input_sequence, columns=[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"], index=input_dates)\n",
    "    output_df = pd.DataFrame(output_sequence, columns=[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"], index=output_dates)\n",
    "\n",
    "    # Concatenate input and output DataFrames\n",
    "    combined_df = pd.concat([input_df, output_df])\n",
    "\n",
    "    # Create a market colors style for the input sequence (standard green/red)\n",
    "    input_market_colors = mpf.make_marketcolors(\n",
    "        up='green', down='red', edge='inherit', wick='inherit', volume='inherit'\n",
    "    )\n",
    "    input_style = mpf.make_mpf_style(marketcolors=input_market_colors)\n",
    "\n",
    "    # Plot the combined data with the input sequence style\n",
    "    fig, axes = mpf.plot(\n",
    "        combined_df,\n",
    "        type='candle',\n",
    "        style=input_style,\n",
    "        volume=True,\n",
    "        mav=(20, 50),\n",
    "        returnfig=True,\n",
    "        title=title,\n",
    "        figsize=(12, 8),\n",
    "        show_nontrading=True\n",
    "    )\n",
    "    ax_main = axes[0]  # main price axis\n",
    "\n",
    "    # Determine the output candlestick color based on label\n",
    "    if label == 'long':\n",
    "        color_up = 'green'\n",
    "        color_down = 'green'\n",
    "    elif label == 'short':\n",
    "        color_up = 'red'\n",
    "        color_down = 'red'\n",
    "    else:  # 'flat'\n",
    "        color_up = 'blue'\n",
    "        color_down = 'blue'\n",
    "\n",
    "    # Create a market colors style for the output sequence\n",
    "    output_market_colors = mpf.make_marketcolors(\n",
    "        up=color_up,\n",
    "        down=color_down,\n",
    "        edge='inherit',\n",
    "        wick='inherit',\n",
    "        volume='inherit'\n",
    "    )\n",
    "    output_style = mpf.make_mpf_style(marketcolors=output_market_colors)\n",
    "\n",
    "    # Overlay the output sequence with its own style\n",
    "    mpf.plot(\n",
    "        output_df,\n",
    "        type='candle',\n",
    "        ax=ax_main,\n",
    "        style=output_style,\n",
    "        volume=False,\n",
    "        mav=(20, 50),\n",
    "        show_nontrading=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat short correct:  model\n",
      "expected/predicted:  flat long correct:  algo\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat long correct:  algo\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  short short correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat long correct:  algo\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  short short correct:  model\n",
      "expected/predicted:  flat short correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  long short correct:  algo\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  long long correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat long correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat long correct:  algo\n",
      "expected/predicted:  long long correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  short short correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat long correct:  algo\n",
      "expected/predicted:  long long correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat short correct:  model\n",
      "expected/predicted:  flat short correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  short short correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat short correct:  algo\n",
      "expected/predicted:  short short correct:  model\n",
      "expected/predicted:  flat long correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat long correct:  model\n",
      "expected/predicted:  long long correct:  model\n",
      "expected/predicted:  flat long correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat long correct:  algo\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  long long correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  long long correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  short short correct:  model\n",
      "expected/predicted:  long long correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  long long correct:  model\n",
      "expected/predicted:  flat short correct:  model\n",
      "expected/predicted:  long flat correct:  algo\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat long correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  long long correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  short short correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  long long correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  long long correct:  model\n",
      "expected/predicted:  long long correct:  model\n",
      "expected/predicted:  short short correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat short correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  long long correct:  model\n",
      "expected/predicted:  flat long correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n",
      "expected/predicted:  flat short correct:  algo\n",
      "expected/predicted:  short short correct:  model\n",
      "expected/predicted:  flat short correct:  model\n",
      "expected/predicted:  flat flat correct:  model\n"
     ]
    }
   ],
   "source": [
    "for idx in random.sample(range(len(X_test_slice)), 100):\n",
    "    sample_idx = X_test_slice[idx]\n",
    "    print(\"expected/predicted: \", labels_test_slice[idx], predicted_labels[idx], \"correct: \", \"model\" if \\\n",
    "        compute_profit(np.expand_dims(X_test_slice[idx], axis=0), np.expand_dims(y_test_slice[idx], axis=0), np.expand_dims(predicted_labels[idx], axis=0))\\\n",
    "        >= \\\n",
    "        compute_profit(np.expand_dims(X_test_slice[idx], axis=0), np.expand_dims(y_test_slice[idx], axis=0), np.expand_dims(labels_test_slice[idx], axis=0))\\\n",
    "        else \"algo\"\n",
    "            )\n",
    "    # plot_input_output_combined_with_label(\n",
    "    #     X_test_slice[idx],\n",
    "    #     y_test_slice[idx],\n",
    "    #     label=predicted_labels[idx],\n",
    "    #     title=\"Input+Output, Colored by Label\"\n",
    "    # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
