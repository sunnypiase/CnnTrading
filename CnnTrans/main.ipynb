{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch.optim as optim\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1. TemporalBlock Class with Corrected Padding\n",
    "# ---------------------------------------------------\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A Temporal Block comprising two dilated convolutional layers with\n",
    "    batch normalization, GELU activations, dropout, and residual connections.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels: int, \n",
    "        out_channels: int, \n",
    "        kernel_size: int, \n",
    "        stride: int, \n",
    "        dilation: int, \n",
    "        padding: int, \n",
    "        dropout: float,\n",
    "        activation: str = 'GELU',\n",
    "        normalization: str = 'BatchNorm'\n",
    "    ):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        \n",
    "        # Select activation function\n",
    "        if activation == 'GELU':\n",
    "            self.activation = nn.GELU()\n",
    "        elif activation == 'LeakyReLU':\n",
    "            self.activation = nn.LeakyReLU()\n",
    "        elif activation == 'ELU':\n",
    "            self.activation = nn.ELU()\n",
    "        else:\n",
    "            self.activation = nn.ReLU()\n",
    "        \n",
    "        # Select normalization layer\n",
    "        if normalization == 'BatchNorm':\n",
    "            self.norm1 = nn.BatchNorm1d(out_channels)\n",
    "            self.norm2 = nn.BatchNorm1d(out_channels)\n",
    "        elif normalization == 'LayerNorm':\n",
    "            self.norm1 = nn.LayerNorm(out_channels)\n",
    "            self.norm2 = nn.LayerNorm(out_channels)\n",
    "        else:\n",
    "            self.norm1 = nn.Identity()\n",
    "            self.norm2 = nn.Identity()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size, \n",
    "            stride=stride, \n",
    "            padding=padding, \n",
    "            dilation=dilation\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            out_channels, \n",
    "            out_channels, \n",
    "            kernel_size, \n",
    "            stride=stride, \n",
    "            padding=padding, \n",
    "            dilation=dilation\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.downsample = nn.Conv1d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size=1\n",
    "        ) if in_channels != out_channels else None\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights using Kaiming Normal for convolutional layers.\n",
    "        'gelu' activation is mapped to 'relu' for initialization purposes.\n",
    "        \"\"\"\n",
    "        # Determine the appropriate nonlinearity for initialization\n",
    "        if isinstance(self.activation, nn.GELU):\n",
    "            init_nonlinearity = 'relu'  # Approximation\n",
    "        elif isinstance(self.activation, nn.LeakyReLU):\n",
    "            init_nonlinearity = 'leaky_relu'\n",
    "        elif isinstance(self.activation, nn.ELU):\n",
    "            init_nonlinearity = 'relu'  # 'ELU' not directly supported\n",
    "        else:\n",
    "            init_nonlinearity = 'relu'  # Default to 'relu'\n",
    "\n",
    "        nn.init.kaiming_normal_(self.conv1.weight, nonlinearity=init_nonlinearity)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight, nonlinearity=init_nonlinearity)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            nn.init.kaiming_normal_(self.downsample.weight, nonlinearity='linear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the Temporal Block.\n",
    "        \"\"\"\n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout2(out)\n",
    "\n",
    "        residual = x if self.downsample is None else self.downsample(x)\n",
    "        return self.activation(out + residual)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2. PositionalEncoding Class\n",
    "# ---------------------------------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the sinusoidal positional encoding for Transformers.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                             (-math.log(10000.0) / d_model))  # (d_model/2,)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)  # (max_len, 1, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (seq_len, batch_size, d_model)\n",
    "        Returns:\n",
    "            Tensor with positional encoding added.\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3. TCNTransformer Model Class\n",
    "# ---------------------------------------------------\n",
    "class TCNTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Convolutional Network combined with Transformer Encoder for Time Series Classification.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_inputs: int, \n",
    "        num_tcn_channels: list, \n",
    "        tcn_kernel_size: int, \n",
    "        tcn_dropout: float, \n",
    "        transformer_hidden_size: int, \n",
    "        transformer_num_heads: int, \n",
    "        transformer_num_layers: int, \n",
    "        transformer_dropout: float, \n",
    "        num_classes: int,\n",
    "        activation: str = 'GELU',\n",
    "        normalization: str = 'BatchNorm',\n",
    "        max_seq_len: int = 1000\n",
    "    ):\n",
    "        super(TCNTransformer, self).__init__()\n",
    "        \n",
    "        # Build TCN layers\n",
    "        layers = []\n",
    "        num_levels = len(num_tcn_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_tcn_channels[i-1]\n",
    "            out_channels = num_tcn_channels[i]\n",
    "            # Correct padding to maintain sequence length\n",
    "            padding = (tcn_kernel_size - 1) * dilation_size // 2\n",
    "            layers += [TemporalBlock(\n",
    "                in_channels, \n",
    "                out_channels, \n",
    "                tcn_kernel_size, \n",
    "                stride=1, \n",
    "                dilation=dilation_size,\n",
    "                padding=padding, \n",
    "                dropout=tcn_dropout,\n",
    "                activation=activation,\n",
    "                normalization=normalization\n",
    "            )]\n",
    "        self.tcn = nn.Sequential(*layers)  # Output shape: (batch_size, num_tcn_channels[-1], seq_len)\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model=num_tcn_channels[-1], max_len=max_seq_len)\n",
    "        \n",
    "        # Projection to transformer hidden size if necessary\n",
    "        if num_tcn_channels[-1] != transformer_hidden_size:\n",
    "            self.projection = nn.Linear(num_tcn_channels[-1], transformer_hidden_size)\n",
    "        else:\n",
    "            self.projection = nn.Identity()\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=transformer_hidden_size, \n",
    "            nhead=transformer_num_heads, \n",
    "            dim_feedforward=transformer_hidden_size * 4, \n",
    "            dropout=transformer_dropout, \n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=transformer_num_layers)\n",
    "        \n",
    "        # Classification Head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(transformer_dropout),\n",
    "            nn.Linear(transformer_hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights for projection and classification layers.\n",
    "        \"\"\"\n",
    "        if not isinstance(self.projection, nn.Identity):\n",
    "            nn.init.xavier_uniform_(self.projection.weight)\n",
    "            nn.init.zeros_(self.projection.bias)\n",
    "        for layer in self.fc:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the TCN-Transformer model.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, num_features, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            Output logits of shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # Pass through TCN\n",
    "        tcn_out = self.tcn(x)  # (batch_size, num_tcn_channels[-1], seq_len)\n",
    "        \n",
    "        # Permute for Transformer: (seq_len, batch_size, num_tcn_channels[-1])\n",
    "        tcn_out = tcn_out.permute(2, 0, 1)\n",
    "        \n",
    "        # Project to transformer hidden size if necessary\n",
    "        transformer_input = self.projection(tcn_out)  # (seq_len, batch_size, transformer_hidden_size)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        transformer_input = self.positional_encoding(transformer_input)  # (seq_len, batch_size, transformer_hidden_size)\n",
    "        \n",
    "        # Pass through Transformer Encoder\n",
    "        transformer_out = self.transformer_encoder(transformer_input)  # (seq_len, batch_size, transformer_hidden_size)\n",
    "        \n",
    "        # Aggregate Transformer outputs (e.g., take the mean over the sequence)\n",
    "        transformer_out = transformer_out.mean(dim=0)  # (batch_size, transformer_hidden_size)\n",
    "        \n",
    "        # Classification Head\n",
    "        out = self.fc(transformer_out)  # (batch_size, num_classes)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4. TimeSeriesDataset Class\n",
    "# ---------------------------------------------------\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequences: Numpy array of shape (num_samples, seq_len, num_features)\n",
    "            labels: Numpy array of shape (num_samples,)\n",
    "        \"\"\"\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Transpose to (num_features, seq_len) for TCN input\n",
    "        return self.sequences[idx].permute(1, 0), self.labels[idx]\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5. Example Usage and Verification\n",
    "# ---------------------------------------------------\n",
    "def main():\n",
    "    # Define model parameters\n",
    "    num_inputs = 10  # Number of features\n",
    "    sequence_length = 100  # Number of time steps (n minutes candles)\n",
    "    batch_size = 32\n",
    "    num_tcn_channels = [64, 128, 256]  # TCN channels per layer\n",
    "    tcn_kernel_size = 3\n",
    "    tcn_dropout = 0.3\n",
    "    transformer_hidden_size = 256  # Must match the last TCN channel or use projection\n",
    "    transformer_num_heads = 8\n",
    "    transformer_num_layers = 3\n",
    "    transformer_dropout = 0.1\n",
    "    num_classes = 3  # long, short, flat\n",
    "    max_seq_len = sequence_length  # Ensure positional encoding covers sequence length\n",
    "\n",
    "    # Initialize the model\n",
    "    model = TCNTransformer(\n",
    "        num_inputs=num_inputs, \n",
    "        num_tcn_channels=num_tcn_channels, \n",
    "        tcn_kernel_size=tcn_kernel_size, \n",
    "        tcn_dropout=tcn_dropout, \n",
    "        transformer_hidden_size=transformer_hidden_size, \n",
    "        transformer_num_heads=transformer_num_heads, \n",
    "        transformer_num_layers=transformer_num_layers, \n",
    "        transformer_dropout=transformer_dropout, \n",
    "        num_classes=num_classes,\n",
    "        activation='GELU',\n",
    "        normalization='BatchNorm',\n",
    "        max_seq_len=max_seq_len\n",
    "    )\n",
    "    \n",
    "    # Verify model architecture\n",
    "    print(model)\n",
    "    \n",
    "    # Create a random input tensor\n",
    "    x = torch.rand(batch_size, num_inputs, sequence_length)  # (batch_size, num_features, seq_len)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)  # (batch_size, num_classes)\n",
    "    print(output.shape)  # Expected: (batch_size, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "main()\n",
    "# train_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Load and Inspect the Data\n",
    "# ------------------------------\n",
    "\n",
    "# Define the file path\n",
    "file_path = '../Data/Binance_BTCUSDT_2024_minute — копия.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Initial Data:\")\n",
    "print(df.head())\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Preprocess the Data\n",
    "# ------------------------------\n",
    "\n",
    "# Convert 'date' column to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Set 'date' as the DataFrame index\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# Sort the DataFrame by datetime index to ensure chronological order\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "# Extract OCHL columns\n",
    "ochl_df = df[['open', 'high', 'low', 'close']].copy()\n",
    "\n",
    "# Calculate volatility as the average of (high - low) over the window\n",
    "ochl_df['volatility'] = ochl_df['high'] - ochl_df['low']\n",
    "ochl_df['avg_volatility'] = ochl_df['volatility'].rolling(window=500).mean()\n",
    "\n",
    "# Resample to hourly data, taking the last closing price as the hour's close\n",
    "hourly_close = ochl_df['close'].resample('h').last()\n",
    "\n",
    "# Align the average volatility with hourly data by resampling\n",
    "hourly_avg_volatility = ochl_df['avg_volatility'].resample('h').last()\n",
    "\n",
    "# Shift the hourly closes by one to get the next hour's close\n",
    "next_hour_close = hourly_close.shift(-1)\n",
    "\n",
    "# Create a DataFrame to hold the necessary data\n",
    "label_df = pd.DataFrame({\n",
    "    'current_close': hourly_close,\n",
    "    'next_close': next_hour_close,\n",
    "    'avg_volatility': hourly_avg_volatility\n",
    "})\n",
    "\n",
    "# Drop rows with NaN values (especially the last row where next_close is NaN)\n",
    "label_df.dropna(inplace=True)\n",
    "\n",
    "# Define labels based on the criteria\n",
    "def assign_label(row):\n",
    "    if row['next_close'] > row['current_close'] + row['avg_volatility']:\n",
    "        return 'long'\n",
    "    elif row['next_close'] < row['current_close'] - row['avg_volatility']:\n",
    "        return 'short'\n",
    "    else:\n",
    "        return 'flat'\n",
    "\n",
    "label_df['label'] = label_df.apply(assign_label, axis=1)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Generate Input Sequences\n",
    "# ------------------------------\n",
    "\n",
    "# Initialize lists to hold sequences and labels\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "# Iterate over each row in label_df\n",
    "for idx, row in label_df.iterrows():\n",
    "    # Current hour's end time\n",
    "    current_hour_end = idx\n",
    "    \n",
    "    # Define the start time of the 500-minute window\n",
    "    start_time = current_hour_end - timedelta(minutes=500)\n",
    "    \n",
    "    # Check if the start_time exists in the minute-level data\n",
    "    if start_time in ochl_df.index:\n",
    "        # Extract the 500-minute window\n",
    "        window_data = ochl_df.loc[start_time:current_hour_end - timedelta(minutes=1), ['open', 'high', 'low', 'close']]\n",
    "        \n",
    "        # Ensure the window has exactly 500 data points\n",
    "        if len(window_data) == 500:\n",
    "            # Convert to numpy array\n",
    "            window_array = window_data.values  # Shape: (500, 4)\n",
    "            sequences.append(window_array)\n",
    "            labels.append(row['label'])\n",
    "    else:\n",
    "        # If start_time not in index, skip this sample\n",
    "        continue\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "sequences = np.array(sequences)  # Shape: (num_samples, 500, 4)\n",
    "labels = np.array(labels)        # Shape: (num_samples,)\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Encode Labels\n",
    "# ------------------------------\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform labels\n",
    "encoded_labels = le.fit_transform(labels)\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Split the Data\n",
    "# ------------------------------\n",
    "\n",
    "# First, split into training and temporary sets (80% train, 20% temp)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    sequences, encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels\n",
    ")\n",
    "\n",
    "# Then, split the temporary set into validation and test sets (10% each of the total data)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Create PyTorch Datasets and DataLoaders\n",
    "# ------------------------------\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequences (numpy.ndarray): Array of shape (num_samples, seq_len, num_features)\n",
    "            labels (numpy.ndarray): Array of shape (num_samples,)\n",
    "        \"\"\"\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "val_dataset = TimeSeriesDataset(X_val, y_val)\n",
    "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoader instances\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Verify the DataLoaders\n",
    "# ------------------------------\n",
    "\n",
    "# Get a sample batch from the training loader\n",
    "sample_sequence, sample_label = next(iter(train_loader))\n",
    "print(f\"Sample sequence shape: {sample_sequence.shape}\")  # Expected: (batch_size, 500, 4)\n",
    "print(f\"Sample label shape: {sample_label.shape}\")        # Expected: (batch_size,)\n",
    "print(f\"Sample labels: {sample_label}\")                  # Tensor of labels\n",
    "\n",
    "# ------------------------------\n",
    "# 8. Summary of Label Encoding\n",
    "# ------------------------------\n",
    "\n",
    "print(f\"Classes: {le.classes_}\")  # ['flat', 'long', 'short']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters (ensure these match your data)\n",
    "num_inputs = 4  # OCHL features\n",
    "num_tcn_channels = [64, 128, 256]\n",
    "tcn_kernel_size = 3\n",
    "tcn_dropout = 0.3\n",
    "transformer_hidden_size = 256\n",
    "transformer_num_heads = 8\n",
    "transformer_num_layers = 3\n",
    "transformer_dropout = 0.1\n",
    "num_classes = 3  # 'long', 'short', 'flat'\n",
    "max_seq_len = 500  # Number of minutes in the input sequence\n",
    "\n",
    "# Initialize the model\n",
    "model = TCNTransformer(\n",
    "    num_inputs=num_inputs, \n",
    "    num_tcn_channels=num_tcn_channels, \n",
    "    tcn_kernel_size=tcn_kernel_size, \n",
    "    tcn_dropout=tcn_dropout, \n",
    "    transformer_hidden_size=transformer_hidden_size, \n",
    "    transformer_num_heads=transformer_num_heads, \n",
    "    transformer_num_layers=transformer_num_layers, \n",
    "    transformer_dropout=transformer_dropout, \n",
    "    num_classes=num_classes,\n",
    "    activation='GELU',\n",
    "    normalization='BatchNorm',\n",
    "    max_seq_len=max_seq_len\n",
    ")\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Reshape the training sequences to (num_samples * seq_len, num_features) for fitting\n",
    "X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "scaler.fit(X_train_reshaped)\n",
    "\n",
    "# Define a function to scale sequences\n",
    "def scale_sequences(sequences, scaler):\n",
    "    num_samples, seq_len, num_features = sequences.shape\n",
    "    sequences_reshaped = sequences.reshape(-1, num_features)\n",
    "    sequences_scaled = scaler.transform(sequences_reshaped)\n",
    "    return sequences_scaled.reshape(num_samples, seq_len, num_features)\n",
    "\n",
    "# Apply the scaler to all datasets\n",
    "X_train_scaled = scale_sequences(X_train, scaler)\n",
    "X_val_scaled = scale_sequences(X_val, scaler)\n",
    "X_test_scaled = scale_sequences(X_test, scaler)\n",
    "\n",
    "# Update the datasets with scaled data\n",
    "train_dataset = TimeSeriesDataset(X_train_scaled, y_train)\n",
    "val_dataset = TimeSeriesDataset(X_val_scaled, y_val)\n",
    "test_dataset = TimeSeriesDataset(X_test_scaled, y_test)\n",
    "\n",
    "# Recreate DataLoaders if necessary\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify scaling on training data\n",
    "print(\"Scaled training data statistics:\")\n",
    "print(f\"Mean: {X_train_scaled.mean():.4f}\")\n",
    "print(f\"Std: {X_train_scaled.std():.4f}\")\n",
    "\n",
    "# Output should be approximately:\n",
    "# Mean: 0.0000\n",
    "# Std: 1.0000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Compute class weights to handle class imbalance\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "# Convert class weights to a tensor\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Define the loss function with class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.5, \n",
    "    patience=5, \n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 50\n",
    "\n",
    "# Path to save the best model\n",
    "checkpoint_path = 'best_tcn_transformer_model.pth'\n",
    "\n",
    "# Initialize variables to track the best validation F1-Score\n",
    "best_f1 = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # -------------------\n",
    "    # Training Phase\n",
    "    # -------------------\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for sequences_batch, labels_batch in train_loader:\n",
    "        # Move data to device\n",
    "        sequences_batch = sequences_batch.to(device)  # Shape: (batch_size, 500, 4)\n",
    "        labels_batch = labels_batch.to(device)        # Shape: (batch_size,)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequences_batch)  # Shape: (batch_size, 3)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels_batch)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item() * sequences_batch.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    # -------------------\n",
    "    # Validation Phase\n",
    "    # -------------------\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences_batch, labels_batch in val_loader:\n",
    "            # Move data to device\n",
    "            sequences_batch = sequences_batch.to(device)\n",
    "            labels_batch = labels_batch.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(sequences_batch)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels_batch)\n",
    "            val_loss += loss.item() * sequences_batch.size(0)\n",
    "            \n",
    "            # Predictions\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels_batch.cpu().numpy())\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    # Compute F1-Score\n",
    "    report = classification_report(all_labels, all_preds, target_names=le.classes_, output_dict=True)\n",
    "    f1_score_val = report['weighted avg']['f1-score']\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | Val F1-Score: {f1_score_val:.4f}\")\n",
    "    \n",
    "    # Adjust learning rate based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save the model if it has the best F1-Score so far\n",
    "    if f1_score_val > best_f1:\n",
    "        best_f1 = f1_score_val\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved Best Model at Epoch {epoch+1} with F1-Score: {f1_score_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "# Move model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequences_batch, labels_batch in test_loader:\n",
    "        # Move data to device\n",
    "        sequences_batch = sequences_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequences_batch)\n",
    "        \n",
    "        # Predictions\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Compute classification report\n",
    "class_report = classification_report(all_labels, all_preds, target_names=le.classes_)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1. TemporalBlock Class with Corrected Padding\n",
    "# ---------------------------------------------------\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A Temporal Block comprising two dilated convolutional layers with\n",
    "    batch normalization, GELU activations, dropout, and residual connections.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels: int, \n",
    "        out_channels: int, \n",
    "        kernel_size: int, \n",
    "        stride: int, \n",
    "        dilation: int, \n",
    "        padding: int, \n",
    "        dropout: float,\n",
    "        activation: str = 'GELU',\n",
    "        normalization: str = 'BatchNorm'\n",
    "    ):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        \n",
    "        # Select activation function\n",
    "        if activation == 'GELU':\n",
    "            self.activation = nn.GELU()\n",
    "        elif activation == 'LeakyReLU':\n",
    "            self.activation = nn.LeakyReLU()\n",
    "        elif activation == 'ELU':\n",
    "            self.activation = nn.ELU()\n",
    "        else:\n",
    "            self.activation = nn.ReLU()\n",
    "        \n",
    "        # Select normalization layer\n",
    "        if normalization == 'BatchNorm':\n",
    "            self.norm1 = nn.BatchNorm1d(out_channels)\n",
    "            self.norm2 = nn.BatchNorm1d(out_channels)\n",
    "        elif normalization == 'LayerNorm':\n",
    "            self.norm1 = nn.LayerNorm(out_channels)\n",
    "            self.norm2 = nn.LayerNorm(out_channels)\n",
    "        else:\n",
    "            self.norm1 = nn.Identity()\n",
    "            self.norm2 = nn.Identity()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size, \n",
    "            stride=stride, \n",
    "            padding=padding, \n",
    "            dilation=dilation\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            out_channels, \n",
    "            out_channels, \n",
    "            kernel_size, \n",
    "            stride=stride, \n",
    "            padding=padding, \n",
    "            dilation=dilation\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.downsample = nn.Conv1d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size=1\n",
    "        ) if in_channels != out_channels else None\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights using Kaiming Normal for convolutional layers.\n",
    "        'gelu' activation is mapped to 'relu' for initialization purposes.\n",
    "        \"\"\"\n",
    "        # Determine the appropriate nonlinearity for initialization\n",
    "        if isinstance(self.activation, nn.GELU):\n",
    "            init_nonlinearity = 'relu'  # Approximation\n",
    "        elif isinstance(self.activation, nn.LeakyReLU):\n",
    "            init_nonlinearity = 'leaky_relu'\n",
    "        elif isinstance(self.activation, nn.ELU):\n",
    "            init_nonlinearity = 'relu'  # 'ELU' not directly supported\n",
    "        else:\n",
    "            init_nonlinearity = 'relu'  # Default to 'relu'\n",
    "\n",
    "        nn.init.kaiming_normal_(self.conv1.weight, nonlinearity=init_nonlinearity)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight, nonlinearity=init_nonlinearity)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            nn.init.kaiming_normal_(self.downsample.weight, nonlinearity='linear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the Temporal Block.\n",
    "        \"\"\"\n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout2(out)\n",
    "\n",
    "        residual = x if self.downsample is None else self.downsample(x)\n",
    "        return self.activation(out + residual)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2. PositionalEncoding Class\n",
    "# ---------------------------------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the sinusoidal positional encoding for Transformers.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                             (-math.log(10000.0) / d_model))  # (d_model/2,)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)  # (max_len, 1, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (seq_len, batch_size, d_model)\n",
    "        Returns:\n",
    "            Tensor with positional encoding added.\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3. TCNTransformer Model Class\n",
    "# ---------------------------------------------------\n",
    "class TCNTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Convolutional Network combined with Transformer Encoder for Time Series Classification.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_inputs: int, \n",
    "        num_tcn_channels: list, \n",
    "        tcn_kernel_size: int, \n",
    "        tcn_dropout: float, \n",
    "        transformer_hidden_size: int, \n",
    "        transformer_num_heads: int, \n",
    "        transformer_num_layers: int, \n",
    "        transformer_dropout: float, \n",
    "        num_classes: int,\n",
    "        activation: str = 'GELU',\n",
    "        normalization: str = 'BatchNorm',\n",
    "        max_seq_len: int = 1000\n",
    "    ):\n",
    "        super(TCNTransformer, self).__init__()\n",
    "        \n",
    "        # Build TCN layers\n",
    "        layers = []\n",
    "        num_levels = len(num_tcn_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_tcn_channels[i-1]\n",
    "            out_channels = num_tcn_channels[i]\n",
    "            # Correct padding to maintain sequence length\n",
    "            padding = (tcn_kernel_size - 1) * dilation_size // 2\n",
    "            layers += [TemporalBlock(\n",
    "                in_channels, \n",
    "                out_channels, \n",
    "                tcn_kernel_size, \n",
    "                stride=1, \n",
    "                dilation=dilation_size,\n",
    "                padding=padding, \n",
    "                dropout=tcn_dropout,\n",
    "                activation=activation,\n",
    "                normalization=normalization\n",
    "            )]\n",
    "        self.tcn = nn.Sequential(*layers)  # Output shape: (batch_size, num_tcn_channels[-1], seq_len)\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model=num_tcn_channels[-1], max_len=max_seq_len)\n",
    "        \n",
    "        # Projection to transformer hidden size if necessary\n",
    "        if num_tcn_channels[-1] != transformer_hidden_size:\n",
    "            self.projection = nn.Linear(num_tcn_channels[-1], transformer_hidden_size)\n",
    "        else:\n",
    "            self.projection = nn.Identity()\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=transformer_hidden_size, \n",
    "            nhead=transformer_num_heads, \n",
    "            dim_feedforward=transformer_hidden_size * 4, \n",
    "            dropout=transformer_dropout, \n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=transformer_num_layers)\n",
    "        \n",
    "        # Classification Head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(transformer_dropout),\n",
    "            nn.Linear(transformer_hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights for projection and classification layers.\n",
    "        \"\"\"\n",
    "        if not isinstance(self.projection, nn.Identity):\n",
    "            nn.init.xavier_uniform_(self.projection.weight)\n",
    "            nn.init.zeros_(self.projection.bias)\n",
    "        for layer in self.fc:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the TCN-Transformer model.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, num_features, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            Output logits of shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        # Pass through TCN\n",
    "        tcn_out = self.tcn(x)  # (batch_size, num_tcn_channels[-1], seq_len)\n",
    "        \n",
    "        # Permute for Transformer: (seq_len, batch_size, num_tcn_channels[-1])\n",
    "        tcn_out = tcn_out.permute(2, 0, 1)\n",
    "        \n",
    "        # Project to transformer hidden size if necessary\n",
    "        transformer_input = self.projection(tcn_out)  # (seq_len, batch_size, transformer_hidden_size)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        transformer_input = self.positional_encoding(transformer_input)  # (seq_len, batch_size, transformer_hidden_size)\n",
    "        \n",
    "        # Pass through Transformer Encoder\n",
    "        transformer_out = self.transformer_encoder(transformer_input)  # (seq_len, batch_size, transformer_hidden_size)\n",
    "        \n",
    "        # Aggregate Transformer outputs (e.g., take the mean over the sequence)\n",
    "        transformer_out = transformer_out.mean(dim=0)  # (batch_size, transformer_hidden_size)\n",
    "        \n",
    "        # Classification Head\n",
    "        out = self.fc(transformer_out)  # (batch_size, num_classes)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4. TimeSeriesDataset Class\n",
    "# ---------------------------------------------------\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequences: Numpy array of shape (num_samples, seq_len, num_features)\n",
    "            labels: Numpy array of shape (num_samples,)\n",
    "        \"\"\"\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Transpose to (num_features, seq_len) for TCN input\n",
    "        return self.sequences[idx].permute(1, 0), self.labels[idx]\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5. Example Usage and Verification\n",
    "# ---------------------------------------------------\n",
    "def main():\n",
    "    # Define model parameters\n",
    "    num_inputs = 4  # OCHL features\n",
    "    sequence_length = 500  # Number of time steps (n minutes candles)\n",
    "    batch_size = 32\n",
    "    num_tcn_channels = [64, 128, 256]  # TCN channels per layer\n",
    "    tcn_kernel_size = 3\n",
    "    tcn_dropout = 0.3\n",
    "    transformer_hidden_size = 256  # Must match the last TCN channel or use projection\n",
    "    transformer_num_heads = 8\n",
    "    transformer_num_layers = 3\n",
    "    transformer_dropout = 0.1\n",
    "    num_classes = 3  # long, short, flat\n",
    "    max_seq_len = sequence_length  # Ensure positional encoding covers sequence length\n",
    "\n",
    "    # Initialize the model\n",
    "    model = TCNTransformer(\n",
    "        num_inputs=num_inputs, \n",
    "        num_tcn_channels=num_tcn_channels, \n",
    "        tcn_kernel_size=tcn_kernel_size, \n",
    "        tcn_dropout=tcn_dropout, \n",
    "        transformer_hidden_size=transformer_hidden_size, \n",
    "        transformer_num_heads=transformer_num_heads, \n",
    "        transformer_num_layers=transformer_num_layers, \n",
    "        transformer_dropout=transformer_dropout, \n",
    "        num_classes=num_classes,\n",
    "        activation='GELU',\n",
    "        normalization='BatchNorm',\n",
    "        max_seq_len=max_seq_len\n",
    "    )\n",
    "    \n",
    "    # Verify model architecture\n",
    "    print(model)\n",
    "    \n",
    "    # Create a random input tensor\n",
    "    x = torch.rand(batch_size, num_inputs, sequence_length)  # (batch_size, num_features, seq_len)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(x)  # (batch_size, num_classes)\n",
    "    print(output.shape)  # Expected: (batch_size, num_classes)\n",
    "\n",
    "main()\n",
    "# train_and_evaluate()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Load and Inspect the Data\n",
    "# ------------------------------\n",
    "\n",
    "# Define the file path\n",
    "file_path = '../Data/Binance_BTCUSDT_2024_minute — копия.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Initial Data:\")\n",
    "print(df.head())\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Preprocess the Data\n",
    "# ------------------------------\n",
    "\n",
    "# Convert 'date' column to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Set 'date' as the DataFrame index\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# Sort the DataFrame by datetime index to ensure chronological order\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "# Extract OCHL columns\n",
    "ochl_df = df[['open', 'high', 'low', 'close']].copy()\n",
    "\n",
    "# Calculate volatility as the average of (high - low) over the window\n",
    "ochl_df['volatility'] = ochl_df['high'] - ochl_df['low']\n",
    "ochl_df['avg_volatility'] = ochl_df['volatility'].rolling(window=500).mean()\n",
    "\n",
    "# Resample to hourly data, taking the last closing price as the hour's close\n",
    "hourly_close = ochl_df['close'].resample('h').last()\n",
    "\n",
    "# Align the average volatility with hourly data by resampling\n",
    "hourly_avg_volatility = ochl_df['avg_volatility'].resample('h').last()\n",
    "\n",
    "# Shift the hourly closes by one to get the next hour's close\n",
    "next_hour_close = hourly_close.shift(-1)\n",
    "\n",
    "# Create a DataFrame to hold the necessary data\n",
    "label_df = pd.DataFrame({\n",
    "    'current_close': hourly_close,\n",
    "    'next_close': next_hour_close,\n",
    "    'avg_volatility': hourly_avg_volatility\n",
    "})\n",
    "\n",
    "# Drop rows with NaN values (especially the last row where next_close is NaN)\n",
    "label_df.dropna(inplace=True)\n",
    "\n",
    "# Define labels based on the criteria\n",
    "def assign_label(row):\n",
    "    if row['next_close'] > row['current_close'] + row['avg_volatility']:\n",
    "        return 'long'\n",
    "    elif row['next_close'] < row['current_close'] - row['avg_volatility']:\n",
    "        return 'short'\n",
    "    else:\n",
    "        return 'flat'\n",
    "\n",
    "label_df['label'] = label_df.apply(assign_label, axis=1)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Generate Input Sequences\n",
    "# ------------------------------\n",
    "\n",
    "# Initialize lists to hold sequences and labels\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "# Iterate over each row in label_df\n",
    "for idx, row in label_df.iterrows():\n",
    "    # Current hour's end time\n",
    "    current_hour_end = idx\n",
    "    \n",
    "    # Define the start time of the 500-minute window\n",
    "    start_time = current_hour_end - timedelta(minutes=500)\n",
    "    \n",
    "    # Check if the start_time exists in the minute-level data\n",
    "    if start_time in ochl_df.index:\n",
    "        # Extract the 500-minute window\n",
    "        window_data = ochl_df.loc[start_time:current_hour_end - timedelta(minutes=1), ['open', 'high', 'low', 'close']]\n",
    "        \n",
    "        # Ensure the window has exactly 500 data points\n",
    "        if len(window_data) == 500:\n",
    "            # Convert to numpy array\n",
    "            window_array = window_data.values  # Shape: (500, 4)\n",
    "            sequences.append(window_array)\n",
    "            labels.append(row['label'])\n",
    "    else:\n",
    "        # If start_time not in index, skip this sample\n",
    "        continue\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "sequences = np.array(sequences)  # Shape: (num_samples, 500, 4)\n",
    "labels = np.array(labels)        # Shape: (num_samples,)\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Encode Labels\n",
    "# ------------------------------\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform labels\n",
    "encoded_labels = le.fit_transform(labels)\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Split the Data\n",
    "# ------------------------------\n",
    "\n",
    "# First, split into training and temporary sets (80% train, 20% temp)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    sequences, encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels\n",
    ")\n",
    "\n",
    "# Then, split the temporary set into validation and test sets (10% each of the total data)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Create PyTorch Datasets and DataLoaders\n",
    "# ------------------------------\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequences (numpy.ndarray): Array of shape (num_samples, seq_len, num_features)\n",
    "            labels (numpy.ndarray): Array of shape (num_samples,)\n",
    "        \"\"\"\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "val_dataset = TimeSeriesDataset(X_val, y_val)\n",
    "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoader instances\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Verify the DataLoaders\n",
    "# ------------------------------\n",
    "\n",
    "# Get a sample batch from the training loader\n",
    "sample_sequence, sample_label = next(iter(train_loader))\n",
    "print(f\"Sample sequence shape: {sample_sequence.shape}\")  # Expected: (batch_size, 500, 4)\n",
    "print(f\"Sample label shape: {sample_label.shape}\")        # Expected: (batch_size,)\n",
    "print(f\"Sample labels: {sample_label}\")                  # Tensor of labels\n",
    "\n",
    "# ------------------------------\n",
    "# 8. Summary of Label Encoding\n",
    "# ------------------------------\n",
    "\n",
    "print(f\"Classes: {le.classes_}\")  # ['flat', 'long', 'short']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initial Data:\n",
      "            unix                 date   symbol      open      high       low  \\\n",
      "0  1711583940000  2024-03-27 23:59:00  BTCUSDT  69492.00  69500.00  69430.81   \n",
      "1  1711583880000  2024-03-27 23:58:00  BTCUSDT  69484.59  69516.00  69454.86   \n",
      "2  1711583820000  2024-03-27 23:57:00  BTCUSDT  69548.61  69548.61  69470.58   \n",
      "3  1711583760000  2024-03-27 23:56:00  BTCUSDT  69579.37  69614.96  69536.64   \n",
      "4  1711583700000  2024-03-27 23:55:00  BTCUSDT  69559.99  69637.66  69557.93   \n",
      "\n",
      "      close    volume   volume_from  tradecount  \n",
      "0  69469.99  24.22811  1.682992e+06         944  \n",
      "1  69492.00  41.93422  2.913823e+06        1168  \n",
      "2  69484.58  57.01773  3.963247e+06        1653  \n",
      "3  69548.61  26.47675  1.841978e+06         974  \n",
      "4  69579.37  68.19374  4.746594e+06        3120  \n",
      "\n",
      "Total data points after resampling and rolling: 650381 minutes\n",
      "Total samples after windowing: 10839\n",
      "\n",
      "Number of valid samples: 10830\n",
      "Sequence shape: (500, 4)\n",
      "Labels shape: (10830,)\n",
      "\n",
      "Sample label distribution:\n",
      "long     4692\n",
      "short    4451\n",
      "flat     1687\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Classes: ['flat' 'long' 'short']\n",
      "Encoded labels sample: [1 1 0 0 2 1 2 2 1 1]\n",
      "\n",
      "Example Input Sequence (first 5 minutes):\n",
      "[[16514.06 16517.85 16513.02 16516.82]\n",
      " [16517.09 16517.29 16513.13 16513.64]\n",
      " [16513.64 16514.2  16512.9  16512.91]\n",
      " [16513.38 16514.21 16510.2  16510.34]\n",
      " [16510.34 16512.59 16509.3  16510.68]]\n",
      "Expected Label: long (Encoded: 1)\n",
      "\n",
      "Splitting the data into Training, Validation, and Test sets...\n",
      "Training samples: 8664\n",
      "Validation samples: 1083\n",
      "Test samples: 1083\n",
      "\n",
      "Scaling the data using StandardScaler...\n",
      "Scaled sequences shape: (8664, 500, 4)\n",
      "Scaled sequences shape: (1083, 500, 4)\n",
      "Scaled sequences shape: (1083, 500, 4)\n",
      "\n",
      "Scaled training data statistics:\n",
      "Mean per feature: [ 8.1718275e-07 -8.6290896e-07  5.5485572e-07  7.2603075e-07]\n",
      "Std per feature: [0.9979483  0.9979271  0.99791586 0.99792314]\n",
      "\n",
      "Scaled validation data statistics:\n",
      "Mean per feature: [-0.00601726 -0.00599524 -0.00603424 -0.00600832]\n",
      "Std per feature: [0.98535025 0.98534656 0.98530823 0.98532903]\n",
      "\n",
      "Scaled test data statistics:\n",
      "Mean per feature: [0.01532414 0.01536925 0.01529789 0.01534198]\n",
      "Std per feature: [1.0106862 1.0107933 1.0107187 1.0107442]\n",
      "\n",
      "Number of 'flat' class samples before augmentation: 1349\n",
      "Number of 'flat' class samples after augmentation: 2698\n",
      "\n",
      "Creating Dataset instances...\n",
      "\n",
      "Using batch size: 32\n",
      "Creating DataLoader instances...\n",
      "\n",
      "Initializing the model...\n",
      "Model moved to device.\n",
      "\n",
      "Computed Class Weights: [1.2370892  0.88909608 0.93728353]\n",
      "Normalized Class Weights (Alpha): [0.40381975 0.29022527 0.30595498]\n",
      "Defined Focal Loss with class-specific alpha.\n",
      "Initialized AdamW optimizer.\n",
      "Initialized ReduceLROnPlateau scheduler.\n",
      "\n",
      "Starting Training and Evaluation...\n",
      "\n",
      "--- Epoch 1/50 ---\n",
      "\n",
      "An error occurred: DataLoader worker (pid(s) 4172, 8328, 11712, 1700) exited unexpectedly\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------\n",
    "# 1. Import Necessary Libraries\n",
    "# ---------------------------------------------------\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings  # For handling warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from datetime import timedelta  # For time-based operations\n",
    "\n",
    "# Suppress specific warnings if desired\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2. Define Model Components\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# 2.1 TemporalBlock Class\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A Temporal Block comprising two dilated convolutional layers with\n",
    "    batch normalization, GELU activations, dropout, and residual connections.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels: int, \n",
    "        out_channels: int, \n",
    "        kernel_size: int, \n",
    "        stride: int, \n",
    "        dilation: int, \n",
    "        padding: int, \n",
    "        dropout: float,\n",
    "        activation: str = 'GELU',\n",
    "        normalization: str = 'BatchNorm'\n",
    "    ):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        \n",
    "        # Select activation function\n",
    "        if activation == 'GELU':\n",
    "            self.activation = nn.GELU()\n",
    "        elif activation == 'LeakyReLU':\n",
    "            self.activation = nn.LeakyReLU()\n",
    "        elif activation == 'ELU':\n",
    "            self.activation = nn.ELU()\n",
    "        else:\n",
    "            self.activation = nn.ReLU()\n",
    "        \n",
    "        # Select normalization layer\n",
    "        if normalization == 'BatchNorm':\n",
    "            self.norm1 = nn.BatchNorm1d(out_channels)\n",
    "            self.norm2 = nn.BatchNorm1d(out_channels)\n",
    "        elif normalization == 'LayerNorm':\n",
    "            self.norm1 = nn.LayerNorm(out_channels)\n",
    "            self.norm2 = nn.LayerNorm(out_channels)\n",
    "        else:\n",
    "            self.norm1 = nn.Identity()\n",
    "            self.norm2 = nn.Identity()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size, \n",
    "            stride=stride, \n",
    "            padding=padding, \n",
    "            dilation=dilation\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            out_channels, \n",
    "            out_channels, \n",
    "            kernel_size, \n",
    "            stride=stride, \n",
    "            padding=padding, \n",
    "            dilation=dilation\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.downsample = nn.Conv1d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size=1\n",
    "        ) if in_channels != out_channels else None\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights using Kaiming Normal for convolutional layers.\n",
    "        'gelu' activation is mapped to 'relu' for initialization purposes.\n",
    "        \"\"\"\n",
    "        # Determine the appropriate nonlinearity for initialization\n",
    "        if isinstance(self.activation, nn.GELU):\n",
    "            init_nonlinearity = 'relu'  # Approximation\n",
    "        elif isinstance(self.activation, nn.LeakyReLU):\n",
    "            init_nonlinearity = 'leaky_relu'\n",
    "        elif isinstance(self.activation, nn.ELU):\n",
    "            init_nonlinearity = 'relu'  # 'ELU' not directly supported\n",
    "        else:\n",
    "            init_nonlinearity = 'relu'  # Default to 'relu'\n",
    "\n",
    "        nn.init.kaiming_normal_(self.conv1.weight, nonlinearity=init_nonlinearity)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight, nonlinearity=init_nonlinearity)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            nn.init.kaiming_normal_(self.downsample.weight, nonlinearity='linear')\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the Temporal Block.\n",
    "        \"\"\"\n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout2(out)\n",
    "\n",
    "        residual = x if self.downsample is None else self.downsample(x)\n",
    "        return self.activation(out + residual)\n",
    "\n",
    "# 2.2 PositionalEncoding Class\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the sinusoidal positional encoding for Transformers.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                             (-math.log(10000.0) / d_model))  # (d_model/2,)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)  # (max_len, 1, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (seq_len, batch_size, d_model)\n",
    "        Returns:\n",
    "            Tensor with positional encoding added.\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "# 2.3 Custom TransformerEncoderLayerWithWeights Class\n",
    "class TransformerEncoderLayerWithWeights(nn.TransformerEncoderLayer):\n",
    "    \"\"\"\n",
    "    Custom Transformer Encoder Layer that returns attention weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(TransformerEncoderLayerWithWeights, self).__init__(*args, **kwargs)\n",
    "    \n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None, is_causal=False):\n",
    "        # Set need_weights=True to get attention weights\n",
    "        src2, attn_weights = self.self_attn(src, src, src,\n",
    "                                            attn_mask=src_mask,\n",
    "                                            key_padding_mask=src_key_padding_mask,\n",
    "                                            need_weights=True,\n",
    "                                            is_causal=is_causal)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear1(src)\n",
    "        src2 = self.dropout(self.activation(src2))\n",
    "        src2 = self.linear2(src2)\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src, attn_weights  # Return both output and attention weights\n",
    "\n",
    "# 2.4 TCNTransformerWithWeights Model Class\n",
    "class TCNTransformerWithWeights(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Convolutional Network combined with Transformer Encoder for Time Series Classification.\n",
    "    Returns both output logits and attention weights.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_inputs: int, \n",
    "        num_tcn_channels: list, \n",
    "        tcn_kernel_size: int, \n",
    "        tcn_dropout: float, \n",
    "        transformer_hidden_size: int, \n",
    "        transformer_num_heads: int, \n",
    "        transformer_num_layers: int, \n",
    "        transformer_dropout: float, \n",
    "        num_classes: int,\n",
    "        activation: str = 'GELU',\n",
    "        normalization: str = 'BatchNorm',\n",
    "        max_seq_len: int = 1000\n",
    "    ):\n",
    "        super(TCNTransformerWithWeights, self).__init__()\n",
    "        \n",
    "        # Build TCN layers\n",
    "        layers = []\n",
    "        num_levels = len(num_tcn_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_tcn_channels[i-1]\n",
    "            out_channels = num_tcn_channels[i]\n",
    "            # Correct padding to maintain sequence length\n",
    "            padding = (tcn_kernel_size - 1) * dilation_size // 2\n",
    "            layers += [TemporalBlock(\n",
    "                in_channels, \n",
    "                out_channels, \n",
    "                tcn_kernel_size, \n",
    "                stride=1, \n",
    "                dilation=dilation_size,\n",
    "                padding=padding, \n",
    "                dropout=tcn_dropout,\n",
    "                activation=activation,\n",
    "                normalization=normalization\n",
    "            )]\n",
    "        self.tcn = nn.Sequential(*layers)  # Output shape: (batch_size, num_tcn_channels[-1], seq_len)\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model=num_tcn_channels[-1], max_len=max_seq_len)\n",
    "        \n",
    "        # Projection to transformer hidden size if necessary\n",
    "        if num_tcn_channels[-1] != transformer_hidden_size:\n",
    "            self.projection = nn.Linear(num_tcn_channels[-1], transformer_hidden_size)\n",
    "        else:\n",
    "            self.projection = nn.Identity()\n",
    "        \n",
    "        # Transformer Encoder with custom layers\n",
    "        encoder_layer = TransformerEncoderLayerWithWeights(\n",
    "            d_model=transformer_hidden_size, \n",
    "            nhead=transformer_num_heads, \n",
    "            dim_feedforward=transformer_hidden_size * 4, \n",
    "            dropout=transformer_dropout, \n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=transformer_num_layers)\n",
    "        \n",
    "        # Classification Head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(transformer_dropout),\n",
    "            nn.Linear(transformer_hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights for projection and classification layers.\n",
    "        \"\"\"\n",
    "        if not isinstance(self.projection, nn.Identity):\n",
    "            nn.init.xavier_uniform_(self.projection.weight)\n",
    "            nn.init.zeros_(self.projection.bias)\n",
    "        for layer in self.fc:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the TCN-Transformer model.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, num_features, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            out: Output logits of shape (batch_size, num_classes)\n",
    "            attention_weights: List of attention weights from each Transformer layer\n",
    "        \"\"\"\n",
    "        # Pass through TCN\n",
    "        tcn_out = self.tcn(x)  # (batch_size, num_tcn_channels[-1], seq_len)\n",
    "        \n",
    "        # Permute for Transformer: (seq_len, batch_size, num_tcn_channels[-1])\n",
    "        tcn_out = tcn_out.permute(2, 0, 1)\n",
    "        \n",
    "        # Project to transformer hidden size if necessary\n",
    "        transformer_input = self.projection(tcn_out)  # (seq_len, batch_size, transformer_hidden_size)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        transformer_input = self.positional_encoding(transformer_input)  # (seq_len, batch_size, transformer_hidden_size)\n",
    "        \n",
    "        # Pass through Transformer Encoder and capture attention weights\n",
    "        attention_weights = []\n",
    "        transformer_out = transformer_input\n",
    "        for layer in self.transformer_encoder.layers:\n",
    "            transformer_out, attn_weights = layer(transformer_out)\n",
    "            attention_weights.append(attn_weights)\n",
    "        \n",
    "        # Aggregate Transformer outputs (e.g., take the mean over the sequence)\n",
    "        transformer_out = transformer_out.mean(dim=0)  # (batch_size, transformer_hidden_size)\n",
    "        \n",
    "        # Classification Head\n",
    "        out = self.fc(transformer_out)  # (batch_size, num_classes)\n",
    "        \n",
    "        return out, attention_weights\n",
    "\n",
    "# 2.5 TimeSeriesDataset Class\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sequences: Numpy array of shape (num_samples, seq_len, num_features)\n",
    "            labels: Numpy array of shape (num_samples,)\n",
    "        \"\"\"\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Transpose to (num_features, seq_len) for TCN input\n",
    "        return self.sequences[idx].permute(1, 0), self.labels[idx]\n",
    "\n",
    "# 2.6 Focal Loss Class\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha (tensor, optional): Weights for each class. Shape: (num_classes,)\n",
    "            gamma (float): Focusing parameter.\n",
    "            reduction (str): 'mean' | 'sum' | 'none'\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3. Data Loading and Preprocessing\n",
    "# ---------------------------------------------------\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads data from a CSV file, preprocesses it to generate input sequences and labels.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "        sequences (numpy.ndarray): Array of shape (num_samples, seq_len, num_features)\n",
    "        labels (numpy.ndarray): Array of shape (num_samples,)\n",
    "        label_encoder (LabelEncoder): Fitted label encoder\n",
    "        example_sequence (numpy.ndarray): An example input sequence\n",
    "        example_label (int): Encoded label for the example sequence\n",
    "    \"\"\"\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "    \n",
    "    # Load the data into a pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Display the first few rows\n",
    "    print(\"Initial Data:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Convert 'date' column to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Set 'date' as the DataFrame index\n",
    "    df.set_index('date', inplace=True)\n",
    "    \n",
    "    # Sort the DataFrame by datetime index to ensure chronological order\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    # Ensure the minute-level data has a fixed frequency ('T' for minutes)\n",
    "    ochl_df = df[['open', 'high', 'low', 'close']].copy()\n",
    "    ochl_df = ochl_df.asfreq('T')  # 'T' stands for minute frequency\n",
    "\n",
    "    # Handle missing data by forward filling\n",
    "    ochl_df.ffill(inplace=True)\n",
    "    \n",
    "    # Calculate volatility as the difference between high and low\n",
    "    ochl_df['volatility'] = ochl_df['high'] - ochl_df['low']\n",
    "    \n",
    "    # Calculate average volatility over a rolling window of 500 minutes\n",
    "    ochl_df['avg_volatility'] = ochl_df['volatility'].rolling(window=500).mean()\n",
    "    \n",
    "    # Drop rows with NaN values resulting from rolling window\n",
    "    ochl_df.dropna(inplace=True)\n",
    "    \n",
    "    # Resample to hourly data, taking the last closing price as the hour's close\n",
    "    hourly_close = ochl_df['close'].resample('H').last()\n",
    "    hourly_avg_volatility = ochl_df['avg_volatility'].resample('H').last()\n",
    "    \n",
    "    # Shift the hourly closes by one to get the next hour's close\n",
    "    next_hour_close = hourly_close.shift(-1)\n",
    "    \n",
    "    # Create a DataFrame to hold the necessary data\n",
    "    label_df = pd.DataFrame({\n",
    "        'current_close': hourly_close,\n",
    "        'next_close': next_hour_close,\n",
    "        'avg_volatility': hourly_avg_volatility\n",
    "    })\n",
    "    \n",
    "    # Drop rows with NaN values (especially the last row where next_close is NaN)\n",
    "    label_df.dropna(inplace=True)\n",
    "    \n",
    "    # Define labels based on the criteria\n",
    "    def assign_label(row):\n",
    "        if row['next_close'] > row['current_close'] + row['avg_volatility']:\n",
    "            return 'long'\n",
    "        elif row['next_close'] < row['current_close'] - row['avg_volatility']:\n",
    "            return 'short'\n",
    "        else:\n",
    "            return 'flat'\n",
    "    \n",
    "    label_df['label'] = label_df.apply(assign_label, axis=1)\n",
    "    \n",
    "    # Initialize lists to hold sequences and labels\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    # Print total number of data points\n",
    "    print(f\"\\nTotal data points after resampling and rolling: {len(ochl_df)} minutes\")\n",
    "    print(f\"Total samples after windowing: {len(label_df)}\")\n",
    "    \n",
    "    # Initialize a list to record window sizes (optional, useful for debugging)\n",
    "    window_sizes = []\n",
    "    \n",
    "    # Iterate over each row in label_df\n",
    "    for idx, row in label_df.iterrows():\n",
    "        # Current hour's end time\n",
    "        current_hour_end = idx\n",
    "        \n",
    "        # Define the start time of the 500-minute window\n",
    "        start_time = current_hour_end - timedelta(minutes=500)\n",
    "        \n",
    "        # Extract the 500-minute window\n",
    "        window_data = ochl_df.loc[start_time:current_hour_end - timedelta(minutes=1), ['open', 'high', 'low', 'close']]\n",
    "        \n",
    "        # Record window size\n",
    "        window_size = len(window_data)\n",
    "        window_sizes.append(window_size)\n",
    "        \n",
    "        # Check if the window has exactly 500 data points\n",
    "        if window_size == 500:\n",
    "            # Convert to numpy array\n",
    "            window_array = window_data.values.astype(np.float32)  # Shape: (500, 4), float32\n",
    "            sequences.append(window_array)\n",
    "            labels.append(row['label'])\n",
    "        else:\n",
    "            # If window_data is not exactly 500 minutes, skip this sample\n",
    "            continue  # Removed print statements as per user request\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    sequences = np.array(sequences)  # Shape: (num_samples, 500, 4)\n",
    "    labels = np.array(labels)        # Shape: (num_samples,)\n",
    "    \n",
    "    print(f\"\\nNumber of valid samples: {sequences.shape[0]}\")\n",
    "    print(f\"Sequence shape: {sequences.shape[1:]}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"\\nSample label distribution:\")\n",
    "    print(pd.Series(labels).value_counts())\n",
    "    \n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    encoded_labels = le.fit_transform(labels)\n",
    "    print(f\"\\nClasses: {le.classes_}\")\n",
    "    print(f\"Encoded labels sample: {encoded_labels[:10]}\")\n",
    "    \n",
    "    # Print an example input and expected result\n",
    "    example_idx = 0  # You can change this index to view different samples\n",
    "    example_sequence = sequences[example_idx]  # Shape: (500, 4)\n",
    "    example_label = encoded_labels[example_idx]\n",
    "    print(\"\\nExample Input Sequence (first 5 minutes):\")\n",
    "    print(example_sequence[:5])  # Display first 5 minutes for brevity\n",
    "    print(f\"Expected Label: {labels[example_idx]} (Encoded: {example_label})\")\n",
    "    \n",
    "    return sequences, encoded_labels, le, example_sequence, example_label\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4. Scaling the Data\n",
    "# ---------------------------------------------------\n",
    "def scale_data(X_train, X_val, X_test):\n",
    "    \"\"\"\n",
    "    Scales the data using StandardScaler.\n",
    "    \n",
    "    Args:\n",
    "        X_train (numpy.ndarray): Training data of shape (num_samples, seq_len, num_features)\n",
    "        X_val (numpy.ndarray): Validation data of shape (num_samples, seq_len, num_features)\n",
    "        X_test (numpy.ndarray): Test data of shape (num_samples, seq_len, num_features)\n",
    "    \n",
    "    Returns:\n",
    "        X_train_scaled, X_val_scaled, X_test_scaled: Scaled datasets\n",
    "        scaler: Fitted StandardScaler\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Reshape to (num_samples * seq_len, num_features) for fitting\n",
    "    X_train_reshaped = X_train.reshape(-1, X_train.shape[-1]).astype(np.float32)\n",
    "    \n",
    "    # Fit the scaler on the training data\n",
    "    scaler.fit(X_train_reshaped)\n",
    "    print(\"\\nScaling the data using StandardScaler...\")\n",
    "    \n",
    "    # Define a function to scale sequences\n",
    "    def scale_sequences(sequences, scaler):\n",
    "        num_samples, seq_len, num_features = sequences.shape\n",
    "        sequences_reshaped = sequences.reshape(-1, num_features).astype(np.float32)\n",
    "        sequences_scaled = scaler.transform(sequences_reshaped)\n",
    "        scaled = sequences_scaled.reshape(num_samples, seq_len, num_features).astype(np.float32)\n",
    "        print(f\"Scaled sequences shape: {scaled.shape}\")\n",
    "        return scaled\n",
    "    \n",
    "    # Apply the scaler to all datasets\n",
    "    X_train_scaled = scale_sequences(X_train, scaler)\n",
    "    X_val_scaled = scale_sequences(X_val, scaler)\n",
    "    X_test_scaled = scale_sequences(X_test, scaler)\n",
    "    \n",
    "    # Verify scaling\n",
    "    print(\"\\nScaled training data statistics:\")\n",
    "    print(f\"Mean per feature: {X_train_scaled.mean(axis=(0,1))}\")\n",
    "    print(f\"Std per feature: {X_train_scaled.std(axis=(0,1))}\")\n",
    "    \n",
    "    # Additional statistics for validation and test sets\n",
    "    print(\"\\nScaled validation data statistics:\")\n",
    "    print(f\"Mean per feature: {X_val_scaled.mean(axis=(0,1))}\")\n",
    "    print(f\"Std per feature: {X_val_scaled.std(axis=(0,1))}\")\n",
    "    \n",
    "    print(\"\\nScaled test data statistics:\")\n",
    "    print(f\"Mean per feature: {X_test_scaled.mean(axis=(0,1))}\")\n",
    "    print(f\"Std per feature: {X_test_scaled.std(axis=(0,1))}\")\n",
    "    \n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled, scaler\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5. Addressing Class Imbalance\n",
    "# ---------------------------------------------------\n",
    "def create_weighted_sampler(y_train):\n",
    "    \"\"\"\n",
    "    Creates a WeightedRandomSampler to handle class imbalance.\n",
    "    \n",
    "    Args:\n",
    "        y_train (numpy.ndarray): Training labels.\n",
    "    \n",
    "    Returns:\n",
    "        sampler (WeightedRandomSampler): Sampler for DataLoader.\n",
    "    \"\"\"\n",
    "    class_counts = np.bincount(y_train)\n",
    "    class_weights = 1. / class_counts\n",
    "    samples_weights = class_weights[y_train]\n",
    "    sampler = WeightedRandomSampler(weights=samples_weights, num_samples=len(samples_weights), replacement=True)\n",
    "    return sampler\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 6. Data Augmentation (Optional)\n",
    "# ---------------------------------------------------\n",
    "def augment_data(sequences, noise_factor=0.01):\n",
    "    \"\"\"\n",
    "    Adds Gaussian noise to the sequences for data augmentation.\n",
    "    \n",
    "    Args:\n",
    "        sequences (numpy.ndarray): Original sequences of shape (num_samples, seq_len, num_features)\n",
    "        noise_factor (float): Standard deviation of the Gaussian noise.\n",
    "    \n",
    "    Returns:\n",
    "        augmented_sequences (numpy.ndarray): Augmented sequences.\n",
    "    \"\"\"\n",
    "    noise = np.random.normal(0, noise_factor, sequences.shape).astype(np.float32)\n",
    "    augmented_sequences = sequences + noise\n",
    "    return augmented_sequences\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 7. Training and Evaluation Pipeline\n",
    "# ---------------------------------------------------\n",
    "def train_and_evaluate(model, \n",
    "                       train_loader, \n",
    "                       val_loader, \n",
    "                       test_loader, \n",
    "                       criterion, \n",
    "                       optimizer, \n",
    "                       scheduler, \n",
    "                       device, \n",
    "                       num_epochs=50, \n",
    "                       checkpoint_path='best_tcn_transformer_model.pth',\n",
    "                       patience=10,\n",
    "                       example_input=None,\n",
    "                       example_label=None,\n",
    "                       le=None):  # Added 'le' parameter\n",
    "    \"\"\"\n",
    "    Trains and evaluates the model with early stopping.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The TCNTransformerWithWeights model.\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        val_loader (DataLoader): DataLoader for validation data.\n",
    "        test_loader (DataLoader): DataLoader for test data.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer.\n",
    "        scheduler (torch.optim.lr_scheduler): Learning rate scheduler.\n",
    "        device (torch.device): Device to train on.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        checkpoint_path (str): Path to save the best model.\n",
    "        patience (int): Patience for early stopping.\n",
    "        example_input (numpy.ndarray): Example input sequence for testing.\n",
    "        example_label (int): Encoded label of the example input.\n",
    "        le (LabelEncoder): Label encoder instance.\n",
    "    \n",
    "    Returns:\n",
    "        model (nn.Module): Trained model.\n",
    "    \"\"\"\n",
    "    best_f1 = 0.0\n",
    "    counter = 0\n",
    "    \n",
    "    print(\"\\nStarting Training and Evaluation...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
    "        # -------------------\n",
    "        # Training Phase\n",
    "        # -------------------\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (sequences_batch, labels_batch) in enumerate(train_loader):\n",
    "            # Move data to device\n",
    "            sequences_batch = sequences_batch.to(device)  # Shape: (batch_size, num_features, seq_len)\n",
    "            labels_batch = labels_batch.to(device)        # Shape: (batch_size,)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            try:\n",
    "                # Forward pass\n",
    "                outputs, _ = model(sequences_batch)  # Shape: (batch_size, num_classes), attention_weights\n",
    "            except Exception as e:\n",
    "                print(f\"Error during model forward pass: {e}\")\n",
    "                return\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels_batch)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate loss\n",
    "            running_loss += loss.item() * sequences_batch.size(0)\n",
    "            \n",
    "            if (batch_idx + 1) % 100 == 0 or (batch_idx + 1) == len(train_loader):\n",
    "                print(f\"  Training Batch {batch_idx+1}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f\"  Epoch {epoch+1} Training Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # -------------------\n",
    "        # Validation Phase\n",
    "        # -------------------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (sequences_batch, labels_batch) in enumerate(val_loader):\n",
    "                # Move data to device\n",
    "                sequences_batch = sequences_batch.to(device)\n",
    "                labels_batch = labels_batch.to(device)\n",
    "                \n",
    "                try:\n",
    "                    # Forward pass\n",
    "                    outputs, _ = model(sequences_batch)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during model forward pass (validation): {e}\")\n",
    "                    return\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, labels_batch)\n",
    "                val_loss += loss.item() * sequences_batch.size(0)\n",
    "                \n",
    "                # Predictions\n",
    "                _, preds = torch.max(outputs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels_batch.cpu().numpy())\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        print(f\"  Epoch {epoch+1} Validation Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Compute F1-Score\n",
    "        report = classification_report(all_labels, all_preds, output_dict=True, zero_division=0)\n",
    "        f1_score_val = report['weighted avg']['f1-score']\n",
    "        print(f\"  Epoch {epoch+1} Validation F1-Score: {f1_score_val:.4f}\")\n",
    "        \n",
    "        # Adjust learning rate based on validation loss\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early Stopping and Checkpointing\n",
    "        if f1_score_val > best_f1:\n",
    "            best_f1 = f1_score_val\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"  [Checkpoint] Saved Best Model at Epoch {epoch+1} with F1-Score: {f1_score_val:.4f}\")\n",
    "        else:\n",
    "            counter += 1\n",
    "            print(f\"  Early Stopping Counter: {counter}/{patience}\")\n",
    "            if counter >= patience:\n",
    "                print(\"  [Early Stopping] Triggered.\")\n",
    "                break\n",
    "    \n",
    "    # Load the best model\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(checkpoint_path))\n",
    "        print(\"\\nLoaded Best Model for Testing.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the best model: {e}\")\n",
    "        return\n",
    "    \n",
    "    # -------------------\n",
    "    # Test Phase\n",
    "    # -------------------\n",
    "    if test_loader is not None:\n",
    "        print(\"\\n--- Testing Phase ---\")\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (sequences_batch, labels_batch) in enumerate(test_loader):\n",
    "                # Move data to device\n",
    "                sequences_batch = sequences_batch.to(device)\n",
    "                labels_batch = labels_batch.to(device)\n",
    "                \n",
    "                try:\n",
    "                    # Forward pass\n",
    "                    outputs, _ = model(sequences_batch)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during model forward pass (testing): {e}\")\n",
    "                    return\n",
    "                \n",
    "                # Predictions\n",
    "                _, preds = torch.max(outputs, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels_batch.cpu().numpy())\n",
    "        \n",
    "        # Compute confusion matrix\n",
    "        conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(conf_matrix)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plt.figure(figsize=(8,6))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=['flat', 'long', 'short'],\n",
    "                    yticklabels=['flat', 'long', 'short'])\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "        \n",
    "        # Compute classification report\n",
    "        class_report = classification_report(all_labels, all_preds, target_names=['flat', 'long', 'short'], zero_division=0)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(class_report)\n",
    "    else:\n",
    "        print(\"\\nNo test loader provided. Skipping test phase.\")\n",
    "    \n",
    "    # -------------------\n",
    "    # Print Model Output for Example Input\n",
    "    # -------------------\n",
    "    if example_input is not None and le is not None:\n",
    "        print(\"\\n--- Model Output for Example Input ---\")\n",
    "        try:\n",
    "            # Prepare the example input\n",
    "            example_tensor = torch.tensor(example_input, dtype=torch.float32).unsqueeze(0)  # Shape: (1, 500, 4)\n",
    "            print(f\"Example Input Shape before permute: {example_tensor.shape}\")  # (1, 500, 4)\n",
    "            example_tensor = example_tensor.permute(0, 2, 1).to(device)  # Shape: (1, 4, 500)\n",
    "            print(f\"Example Tensor Shape after permute: {example_tensor.shape}\")  # (1, 4, 500)\n",
    "            \n",
    "            # Display the scaled example input\n",
    "            print(\"\\n--- Scaled Example Input Sequence (first 5 minutes) ---\")\n",
    "            print(example_input[:5])  # Display first 5 minutes of the scaled example input\n",
    "            \n",
    "            # Display the tensor shape and a snippet\n",
    "            print(\"\\n--- Example Tensor Passed to the Model ---\")\n",
    "            print(f\"Tensor Shape: {example_tensor.shape}\")  # Should be (1, 4, 500)\n",
    "            print(example_tensor[0, :, :5])  # Display first 5 time steps for each feature\n",
    "            \n",
    "            # Get model output\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                output_logits, _ = model(example_tensor)\n",
    "                probabilities = F.softmax(output_logits, dim=1).cpu().numpy()\n",
    "                predicted_class = np.argmax(probabilities, axis=1)[0]\n",
    "            \n",
    "            # Print results\n",
    "            expected_label_name = le.inverse_transform([example_label])[0]\n",
    "            predicted_label_name = le.inverse_transform([predicted_class])[0]\n",
    "            print(f\"\\nExpected Label: {expected_label_name} (Encoded: {example_label})\")\n",
    "            print(f\"Model Predicted Class: {predicted_label_name} (Encoded: {predicted_class})\")\n",
    "            print(f\"Probabilities: {probabilities[0]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during example input processing: {e}\")\n",
    "    else:\n",
    "        print(\"No example input provided for model output demonstration.\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 8. Model Interpretability\n",
    "# ---------------------------------------------------\n",
    "def visualize_attention_weights(model, device, X_test_scaled):\n",
    "    \"\"\"\n",
    "    Visualizes attention weights for a sample sequence.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Trained model.\n",
    "        device (torch.device): Device used for computation.\n",
    "        X_test_scaled (numpy.ndarray): Scaled test data.\n",
    "    \"\"\"\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Select a sample from the test set\n",
    "    sample_idx = 0\n",
    "    sample_sequence = X_test_scaled[sample_idx:sample_idx+1]  # Shape: (1, 500, 4)\n",
    "    sample_tensor = torch.tensor(sample_sequence, dtype=torch.float32).to(device)\n",
    "    \n",
    "    try:\n",
    "        # Get model output and attention weights\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs, attn_weights = model(sample_tensor.permute(0, 2, 1))  # (1, num_classes), list of [num_heads, seq_len, seq_len]\n",
    "    except Exception as e:\n",
    "        print(f\"Error during attention weights extraction: {e}\")\n",
    "        return\n",
    "    \n",
    "    if attn_weights:\n",
    "        # For demonstration, visualize attention weights from the first Transformer layer and first head\n",
    "        first_layer_attn = attn_weights[0][0]  # (num_heads, seq_len, seq_len)\n",
    "        \n",
    "        # Move to CPU and convert to numpy\n",
    "        first_layer_attn_np = first_layer_attn.cpu().numpy()\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(first_layer_attn_np, cmap='viridis')\n",
    "        plt.xlabel('Key Positions')\n",
    "        plt.ylabel('Query Positions')\n",
    "        plt.title('Attention Weights for Sample Sequence (Layer 1, Head 1)')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No attention weights captured.\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 9. Execute the Pipeline\n",
    "# ---------------------------------------------------\n",
    "def main_pipeline():\n",
    "    \"\"\"\n",
    "    Executes the full pipeline: data loading, preprocessing, scaling,\n",
    "    handling class imbalance, model training with early stopping,\n",
    "    and evaluation.\n",
    "    \"\"\"\n",
    "    # File path to the CSV data\n",
    "    file_path = '../Data/Binance_BTCUSDT_2024_minute — копия.csv'\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    sequences, encoded_labels, le, example_sequence, example_label = load_and_preprocess_data(file_path)\n",
    "    \n",
    "    # Check if any sequences were created\n",
    "    if sequences.size == 0:\n",
    "        print(\"No valid samples were created. Please check your window extraction logic.\")\n",
    "        return\n",
    "    \n",
    "    # Split the data\n",
    "    print(\"\\nSplitting the data into Training, Validation, and Test sets...\")\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        sequences, encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    print(f\"Training samples: {X_train.shape[0]}\")\n",
    "    print(f\"Validation samples: {X_val.shape[0]}\")\n",
    "    print(f\"Test samples: {X_test.shape[0]}\")\n",
    "    \n",
    "    # Scale the data\n",
    "    X_train_scaled, X_val_scaled, X_test_scaled, scaler = scale_data(X_train, X_val, X_test)\n",
    "    \n",
    "    # Scale the example input using the same scaler\n",
    "    example_sequence_scaled = scaler.transform(example_sequence)  # Shape: (500, 4)\n",
    "    # Do NOT reshape here; let the train_and_evaluate function handle batching\n",
    "    \n",
    "    # Optional: Data Augmentation (e.g., Jittering) for Minority Classes\n",
    "    # Uncomment the following lines to apply data augmentation\n",
    "    \n",
    "    # Identify the index for the 'flat' class\n",
    "    flat_class_index = np.where(le.classes_ == 'flat')[0][0]\n",
    "    flat_indices = np.where(y_train == flat_class_index)[0]\n",
    "    flat_sequences = X_train_scaled[flat_indices]\n",
    "    flat_labels = y_train[flat_indices]\n",
    "    \n",
    "    print(f\"\\nNumber of 'flat' class samples before augmentation: {len(flat_sequences)}\")\n",
    "    \n",
    "    # Augment 'flat' sequences\n",
    "    augmented_flat_sequences = augment_data(flat_sequences, noise_factor=0.01)\n",
    "    augmented_flat_labels = flat_labels.copy()\n",
    "    \n",
    "    # Combine augmented data with original training data\n",
    "    X_train_combined = np.concatenate((X_train_scaled, augmented_flat_sequences), axis=0)\n",
    "    y_train_combined = np.concatenate((y_train, augmented_flat_labels), axis=0)\n",
    "    \n",
    "    print(f\"Number of 'flat' class samples after augmentation: {len(augmented_flat_sequences) + len(flat_sequences)}\")\n",
    "    \n",
    "    # Create WeightedRandomSampler to address class imbalance\n",
    "    sampler = create_weighted_sampler(y_train_combined)\n",
    "    \n",
    "    # Create Dataset instances\n",
    "    print(\"\\nCreating Dataset instances...\")\n",
    "    train_dataset = TimeSeriesDataset(X_train_combined, y_train_combined)\n",
    "    val_dataset = TimeSeriesDataset(X_val_scaled, y_val)\n",
    "    test_dataset = TimeSeriesDataset(X_test_scaled, y_test)\n",
    "    \n",
    "    # Determine optimal batch size based on GPU memory\n",
    "    # Starting with a smaller batch size to prevent memory issues\n",
    "    batch_size = 32\n",
    "    print(f\"\\nUsing batch size: {batch_size}\")\n",
    "    \n",
    "    # Create DataLoader instances with sampler for training\n",
    "    print(\"Creating DataLoader instances...\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    # Define model parameters\n",
    "    num_inputs = 4  # OCHL features\n",
    "    num_tcn_channels = [64, 128, 256]  # TCN channels per layer\n",
    "    tcn_kernel_size = 3\n",
    "    tcn_dropout = 0.3\n",
    "    transformer_hidden_size = 256  # Must match the last TCN channel or use projection\n",
    "    transformer_num_heads = 8\n",
    "    transformer_num_layers = 3\n",
    "    transformer_dropout = 0.1\n",
    "    num_classes = 3  # long, short, flat\n",
    "    max_seq_len = 500  # Number of minutes in the input sequence\n",
    "    \n",
    "    # Initialize the model\n",
    "    print(\"\\nInitializing the model...\")\n",
    "    model = TCNTransformerWithWeights(\n",
    "        num_inputs=num_inputs, \n",
    "        num_tcn_channels=num_tcn_channels, \n",
    "        tcn_kernel_size=tcn_kernel_size, \n",
    "        tcn_dropout=tcn_dropout, \n",
    "        transformer_hidden_size=transformer_hidden_size, \n",
    "        transformer_num_heads=transformer_num_heads, \n",
    "        transformer_num_layers=transformer_num_layers, \n",
    "        transformer_dropout=transformer_dropout, \n",
    "        num_classes=num_classes,\n",
    "        activation='GELU',\n",
    "        normalization='BatchNorm',\n",
    "        max_seq_len=max_seq_len\n",
    "    )\n",
    "    \n",
    "    # Move the model to the appropriate device\n",
    "    model.to(device)\n",
    "    print(\"Model moved to device.\")\n",
    "    \n",
    "    # Compute class weights based on training data\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train_combined),\n",
    "        y=y_train_combined\n",
    "    )\n",
    "    print(f\"\\nComputed Class Weights: {class_weights}\")\n",
    "    \n",
    "    # Normalize the class weights\n",
    "    alpha = class_weights / class_weights.sum()\n",
    "    print(f\"Normalized Class Weights (Alpha): {alpha}\")\n",
    "    \n",
    "    # Convert class weights to tensor\n",
    "    alpha_tensor = torch.tensor(alpha, dtype=torch.float).to(device)\n",
    "    \n",
    "    # Define the loss function with class-specific alpha\n",
    "    criterion = FocalLoss(alpha=alpha_tensor, gamma=2, reduction='mean')\n",
    "    print(\"Defined Focal Loss with class-specific alpha.\")\n",
    "    # Alternatively, use CrossEntropyLoss with class weights\n",
    "    # criterion = nn.CrossEntropyLoss(weight=alpha_tensor)\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "    print(\"Initialized AdamW optimizer.\")\n",
    "    \n",
    "    # Define a learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5, \n",
    "        patience=5, \n",
    "        verbose=True\n",
    "    )\n",
    "    print(\"Initialized ReduceLROnPlateau scheduler.\")\n",
    "    \n",
    "    # Start training and evaluation\n",
    "    trained_model = train_and_evaluate(\n",
    "        model=model, \n",
    "        train_loader=train_loader, \n",
    "        val_loader=val_loader, \n",
    "        test_loader=test_loader, \n",
    "        criterion=criterion, \n",
    "        optimizer=optimizer, \n",
    "        scheduler=scheduler, \n",
    "        device=device, \n",
    "        num_epochs=50, \n",
    "        checkpoint_path='best_tcn_transformer_model.pth',\n",
    "        patience=10,\n",
    "        example_input=example_sequence_scaled,  # Pass the scaled example input without batch dimension\n",
    "        example_label=example_label,\n",
    "        le=le  # Passed 'le' here\n",
    "    )\n",
    "    \n",
    "    # -------------------\n",
    "    # Model Interpretability\n",
    "    # -------------------\n",
    "    visualize_attention_weights(trained_model, device, X_test_scaled)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Execute the Pipeline\n",
    "# ---------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main_pipeline()\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
